{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06a71c83",
   "metadata": {},
   "source": [
    "# RNN\n",
    "\n",
    "- 특징: 입력과 분석이 시퀀싱한 형태로 이뤄짐. T+1의 계산에 T 기의 데이터가 사용된다. \n",
    "- 사례: 문장이나 시계열 모델이 대표적인 사례.\n",
    "- 계산하는 방식: $h_t$를 계산할때 이전의 state $h_{t-1}$이 들어간다.\n",
    "$$h_t = f_W(h_{t-1}, x_t) $$\n",
    "$h_t$ : new state\n",
    "$f_w$ : some function with param W\n",
    "$H_{t-1}$: old state\n",
    "$x_t$: input vector at some time step\n",
    "- 핵심: x -> RNN -> y 에서 RNN이 자기 자신으로 돌아가는 것으로 표시하는 것은 RNN에서 $h_t$를 계산하는 함수 $f_{w}$가 모든 rnn 셀에 들어간다.\n",
    "\n",
    "## Vanilla RNN\n",
    "\n",
    "$wx$를 두고 생각하면,다음과 같은 식으로 정리할 수 있다.\n",
    " <img src=\" https://wikidocs.net/images/page/22886/rnn_image4_ver2.PNG\">\n",
    "\n",
    " $h_t = f_W(h_{t-1}, x_t)$\n",
    "\n",
    " $h_t = \\tanh(W_{hh}h_{t-1} + W_{xh}x_t)$ \n",
    " \n",
    " $h_t$는 $h_{t-1}$과 $x_t$에 가중치를 부여하고 하이퍼볼릭 탄젠트를 activation function으로 사용한다.\n",
    " \n",
    " $y_t = W_{hy}h_t$\n",
    " \n",
    " y를 뽑을때는 $h_t$에 가중치 $W_{hy}$를 부여한다. $W$의 형태에 따라서, 결과값의 형태가 전부 달라진다.\n",
    " \n",
    " ## 실제 RNN의 계산\n",
    " <img src=\" https://wikidocs.net/images/page/22886/rnn_image2_ver3.PNG\">\n",
    " \n",
    "- 사례: X = [h,e,l,l,o]. training sequence \"hello\" 입력값 x는 onehot vector로 들어가게 된다. \n",
    "- 목표: $x_t$를 넣으면, $y_t$에 \"e\"가 나오는 모델, 미리 앞에 나올 문자를 미리 예측하는 모델을 만드는 것.\n",
    "\n",
    "- $h_t$의 계산: $x_1$에서는 $h_{t-1}$값이 없기 때문에, 0으로 두고 계산한다. $x_t$와 가중치 행렬 $W_{xh}$를 계산하여, $h_t$값을 구한다.\n",
    "- $y_t$의 계산: $h_t$에 가중치 $W_{hy}$를 곱해서 y_t를 구함. y_t에 소프트맥스를 구하면, $y_t$의 결과를 알 수 있음.다만 y값은 마지막 셀에만 들어가는 경우도 있음. 셀의 사용에 따라서 달라진다.\n",
    "- 결과적으로 최종값의 계산에 최초값이후의 값들이 일정정도 영향을 미친다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c944a5e7",
   "metadata": {},
   "source": [
    "## RNN의 활용\n",
    "- 쓰고자 하는 방법에 따라서 Y셀의 형태가 달라진다. \n",
    "<img src=\"http://i.imgur.com/Q8zv6TQ.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c95636",
   "metadata": {},
   "source": [
    "# RNN basic in keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105c16f5",
   "metadata": {},
   "source": [
    "## 단일 레이어 모델\n",
    "<img src=\"https://camo.githubusercontent.com/3bd029379545a119296ea91f94d54eea7bb7396c97c6757839c81c33748404e5/68747470733a2f2f636c6f75642e67697468756275736572636f6e74656e742e636f6d2f6173736574732f3930313937352f32333334383732372f63633938313835362d666365372d313165362d383365612d3462313837343733343636622e706e67\">\n",
    "x_t의 shape는 batch size, sequence lenth, input dimension으로 전처리 되어야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24e72575",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-04T08:07:58.350343Z",
     "start_time": "2022-01-04T08:07:54.867525Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba5ee89e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-04T08:07:59.071676Z",
     "start_time": "2022-01-04T08:07:58.352346Z"
    }
   },
   "outputs": [],
   "source": [
    "# One hot encoding for each char in 'hello'\n",
    "h = [1, 0, 0, 0]\n",
    "e = [0, 1, 0, 0]\n",
    "l = [0, 0, 1, 0]\n",
    "o = [0, 0, 0, 1]\n",
    "\n",
    "x_data = tf.constant([[h]], tf.float32)\n",
    "hidden_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f0697f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-04T08:07:59.671825Z",
     "start_time": "2022-01-04T08:07:59.073676Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data: [[[1. 0. 0. 0.]]], shape: (1, 1, 4)\n",
      "outputs: [[[-0.2983816  -0.49517432]]], shape: (1, 1, 2)\n",
      "states: [[-0.2983816  -0.49517432]], shape: (1, 2)\n"
     ]
    }
   ],
   "source": [
    "## rnn 1번째 방식.\n",
    "cell = layers.SimpleRNNCell(units=hidden_size) # creating SimpleRNNCell\n",
    "rnn = layers.RNN(cell, return_sequences=True, return_state=True) # analogous to tf.nn.dynamic_rnn\n",
    "outputs, states = rnn(x_data)\n",
    "\n",
    "print('x_data: {}, shape: {}'.format(x_data, x_data.shape))\n",
    "print('outputs: {}, shape: {}'.format(outputs, outputs.shape))\n",
    "print('states: {}, shape: {}'.format(states, states.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e63c750b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-04T08:07:59.701839Z",
     "start_time": "2022-01-04T08:07:59.672815Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data: [[[1. 0. 0. 0.]]], shape: (1, 1, 4)\n",
      "outputs: [[[-0.2983816  -0.49517432]]], shape: (1, 1, 2)\n",
      "states: [[0.70752954 0.12894376]], shape: (1, 2)\n"
     ]
    }
   ],
   "source": [
    "# RNN  2번째 방식\n",
    "rnn = layers.SimpleRNN(units = hidden_size, return_sequences = True, return_state = True)\n",
    "ouputs, states = rnn(x_data)\n",
    "\n",
    "print('x_data: {}, shape: {}'.format(x_data, x_data.shape))\n",
    "print('outputs: {}, shape: {}'.format(outputs, outputs.shape))\n",
    "print('states: {}, shape: {}'.format(states, states.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec4aca7",
   "metadata": {},
   "source": [
    "## Sequencing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54f7c4f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-04T08:07:59.716842Z",
     "start_time": "2022-01-04T08:07:59.703819Z"
    }
   },
   "outputs": [],
   "source": [
    "x_data = tf.constant([[h,e,l,l,o]], tf.float32)\n",
    "hidden_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c12793fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-04T08:07:59.746828Z",
     "start_time": "2022-01-04T08:07:59.718832Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data: [[[1. 0. 0. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [0. 0. 0. 1.]]], shape: (1, 5, 4)\n",
      "outputs: [[[ 0.5278045   0.58650917]\n",
      "  [ 0.5523772   0.45844215]\n",
      "  [ 0.87264305 -0.3445563 ]\n",
      "  [ 0.58223146  0.17501253]\n",
      "  [-0.00154522  0.66025525]]], shape: (1, 5, 2)\n",
      "states: [[-0.00154522  0.66025525]], shape: (1, 2)\n"
     ]
    }
   ],
   "source": [
    "rnn = layers.SimpleRNN(units = 2, return_sequences = True, return_state = True)\n",
    "outputs, states = rnn(x_data)\n",
    "print('x_data: {}, shape: {}'.format(x_data, x_data.shape))\n",
    "print('outputs: {}, shape: {}'.format(outputs, outputs.shape))\n",
    "print('states: {}, shape: {}'.format(states, states.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc1baeb",
   "metadata": {},
   "source": [
    "## batching input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d14b9c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-04T08:07:59.776850Z",
     "start_time": "2022-01-04T08:07:59.747829Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data: [[[1. 0. 0. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [0. 0. 0. 1.]]\n",
      "\n",
      " [[0. 1. 0. 0.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [0. 0. 0. 1.]]\n",
      "\n",
      " [[0. 0. 1. 0.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [0. 0. 1. 0.]]], shape: (3, 5, 4)\n",
      "outputs: [[[ 0.5278045   0.58650917]\n",
      "  [ 0.5523772   0.45844215]\n",
      "  [ 0.87264305 -0.3445563 ]\n",
      "  [ 0.58223146  0.17501253]\n",
      "  [-0.00154522  0.66025525]]], shape: (1, 5, 2)\n",
      "states: [[ 0.22335221 -0.39723948]\n",
      " [ 0.8509325  -0.589404  ]\n",
      " [ 0.06450205 -0.39537302]], shape: (3, 2)\n"
     ]
    }
   ],
   "source": [
    "## shape(3,5,2) batch size:3, sequence:5, outputdim: 2\n",
    "x_data = tf.constant([[h,e,l,l,o],\n",
    "                     [e,o,l,l,o],\n",
    "                     [l,l,e,e,l]], tf.float32)\n",
    "hiden_size = 2\n",
    "rnn = layers.SimpleRNN(units = 2, return_sequences = True, return_state = True)\n",
    "output, states = rnn(x_data)\n",
    "print('x_data: {}, shape: {}'.format(x_data, x_data.shape))\n",
    "print('outputs: {}, shape: {}'.format(outputs, outputs.shape))\n",
    "print('states: {}, shape: {}'.format(states, states.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9b9029",
   "metadata": {},
   "source": [
    "- ouuput: minibatch한 대상의 전체 sequnce에 대상하는 hiddendate의 값.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c31bc9",
   "metadata": {},
   "source": [
    "# various rnn\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/b4sus.jpg\">\n",
    "\n",
    "- x와 y의 갯수에 따라서 RNN을 분류할 수 있다.\n",
    "- one to many: 이미지를 입력으로 받아, 캡션을 생성하는 경우\n",
    "- many to many:문장을 입력해서 문장을 배출하는 번역기나, 문장을 입력해서 형태소를 받는 형태소 분석기.\n",
    "- many to one: 단어나 문장의 감정을 분석하는 sentiment classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec35f2b",
   "metadata": {},
   "source": [
    "## many to one\n",
    "\n",
    "<img src=\"https://media.vlpt.us/images/kangtae/post/e3ab2c9f-4067-4502-aea6-e7a458c32ea5/13.rnn_computational_graph(m-t-o).JPG\">\n",
    "\n",
    " - 입력: 문장을 tokenization한 값을 입력으로 받는다.\n",
    " - word embedding: 단어를 onehot으로 입력하면 너무 sparse한 입력이 되기에, 단어를 numeric하게 바꿔줘야 한다.\n",
    " - embedding layer: x에 단어를 vector로 바꿔주는 layer가 존재한다. \n",
    " - 오차 수정: 최후의 $\\hat{y}$값과 y값의 차를 loss로 해서 backpropagation한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de43594",
   "metadata": {},
   "source": [
    "### word sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4dd723bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-04T08:08:00.572456Z",
     "start_time": "2022-01-04T08:07:59.778836Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential, Model\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24dfa863",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-04T08:08:00.587473Z",
     "start_time": "2022-01-04T08:08:00.575464Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', ' ', 'a', 'b', 'd', 'e', 'g', 'o', 'r', 's', 'w']\n",
      "{0: '<pad>', 1: ' ', 2: 'a', 3: 'b', 4: 'd', 5: 'e', 6: 'g', 7: 'o', 8: 'r', 9: 's', 10: 'w'}\n",
      "{'<pad>': 0, ' ': 1, 'a': 2, 'b': 3, 'd': 4, 'e': 5, 'g': 6, 'o': 7, 'r': 8, 's': 9, 'w': 10}\n"
     ]
    }
   ],
   "source": [
    "words = ['good', 'bad', 'worse', 'so good']\n",
    "y_data = [1,0,0,1]\n",
    "\n",
    "## create tokeniation dictionary\n",
    "# batch 단위 연산을 해야하지만, 데이터가 sequence가 다른 경우를 보정하기 위해 pad를 사용함. \n",
    "char_set = ['<pad>'] + sorted(list(set(''.join(words))))\n",
    "idx2char = {idx : char for idx, char in enumerate(char_set)}\n",
    "char2idx = {char : idx for idx, char in enumerate(char_set)}\n",
    "# 문자를 일련번호로 바꾸고, 일련번호의 연결로 단어를 표현한다. ex) good: 6,7,7,4\n",
    "\n",
    "print(char_set)\n",
    "print(idx2char)\n",
    "print(char2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "049f2644",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-04T08:08:00.602470Z",
     "start_time": "2022-01-04T08:08:00.589462Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6, 7, 7, 4], [3, 2, 4], [10, 7, 8, 9, 5], [9, 7, 1, 6, 7, 7, 4]]\n",
      "[4, 3, 5, 7]\n"
     ]
    }
   ],
   "source": [
    "# converting sequence of tokens to sequence of indices\n",
    "x_data = list(map(lambda word : [char2idx.get(char) for char in word], words))\n",
    "x_data_len = list(map(lambda word : len(word), x_data))\n",
    "\n",
    "print(x_data)\n",
    "print(x_data_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9ba5209",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-04T08:08:00.617473Z",
     "start_time": "2022-01-04T08:08:00.604471Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6  7  7  4  0  0  0  0  0  0]\n",
      " [ 3  2  4  0  0  0  0  0  0  0]\n",
      " [10  7  8  9  5  0  0  0  0  0]\n",
      " [ 9  7  1  6  7  7  4  0  0  0]]\n",
      "[4, 3, 5, 7]\n",
      "[1, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "# padding the sequence of indices\n",
    "# 단어들을 padding 해야함. 0 값으로 padding하는게 기본\n",
    "max_sequence = 10\n",
    "x_data = pad_sequences(sequences = x_data, maxlen = max_sequence,\n",
    "                       padding = 'post', truncating = 'post')\n",
    "\n",
    "# checking data\n",
    "print(x_data)\n",
    "print(x_data_len)\n",
    "print(y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89afd47f",
   "metadata": {},
   "source": [
    "### create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46f55f3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-04T08:08:00.632483Z",
     "start_time": "2022-01-04T08:08:00.619461Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "722eee97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-04T08:08:00.737501Z",
     "start_time": "2022-01-04T08:08:00.634463Z"
    }
   },
   "outputs": [],
   "source": [
    "input_dim = len(char2idx)\n",
    "output_dim = len(char2idx)\n",
    "one_hot = np.eye(len(char2idx))\n",
    "hidden_size = 10\n",
    "num_classes = 2\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(input_dim=input_dim, output_dim = output_dim,\n",
    "                           trainable = False, mask_zero = True, input_length = max_sequence, \n",
    "                           embeddings_initializer = keras.initializers.Constant(one_hot)))\n",
    "model.add(layers.SimpleRNN(units = hidden_size))\n",
    "model.add(layers.Dense(units = num_classes)) # many to one 에서 one에 해당한다.\n",
    "# mask_zero = True 전처리 단계에서 패딩 된 부분은 연산에서 제외\n",
    "# trainable true: onehot을 train하지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd84f554",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-04T08:08:00.752510Z",
     "start_time": "2022-01-04T08:08:00.738495Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 10, 11)            121       \n",
      "_________________________________________________________________\n",
      "simple_rnn_3 (SimpleRNN)     (None, 10)                220       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 363\n",
      "Trainable params: 242\n",
      "Non-trainable params: 121\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e2da6ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-04T08:08:00.767507Z",
     "start_time": "2022-01-04T08:08:00.754498Z"
    }
   },
   "outputs": [],
   "source": [
    "## create loss function\n",
    "\n",
    "def loss_fn(model, x, y):\n",
    "    return tf.compat.v1.losses.sparse_softmax_cross_entropy(labels = y, logits = model(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5484c845",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-04T08:08:00.782504Z",
     "start_time": "2022-01-04T08:08:00.769503Z"
    }
   },
   "outputs": [],
   "source": [
    "## create optimizer\n",
    "lr = .01\n",
    "epochs = 30\n",
    "batch_size = 2\n",
    "opt = tf.keras.optimizers.Adam(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "072e8b99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-04T08:08:00.797507Z",
     "start_time": "2022-01-04T08:08:00.784505Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((None, 10), (None,)), types: (tf.int32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "## generate pipeline\n",
    "tr_dataset = tf.data.Dataset.from_tensor_slices((x_data, y_data)) # dataset instance\n",
    "tr_dataset = tr_dataset.shuffle(buffer_size = 4)\n",
    "tr_dataset = tr_dataset.batch(batch_size = batch_size)\n",
    "\n",
    "print(tr_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1579695c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-04T08:08:03.108032Z",
     "start_time": "2022-01-04T08:08:00.799509Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 tf.Tensor(0.23411113, shape=(), dtype=float32)\n",
      "9 tf.Tensor(0.037457384, shape=(), dtype=float32)\n",
      "14 tf.Tensor(0.011339604, shape=(), dtype=float32)\n",
      "19 tf.Tensor(0.005884296, shape=(), dtype=float32)\n",
      "24 tf.Tensor(0.004054619, shape=(), dtype=float32)\n",
      "29 tf.Tensor(0.0031857477, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "## training\n",
    "tr_loss_hist = []\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    avg_tr_loss = 0\n",
    "    tr_step = 0\n",
    "    \n",
    "    for x_mb, y_mb in tr_dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            tr_loss = loss_fn(model, x = x_mb, y = y_mb) ## minibatch 마다의 cross entropy\n",
    "        grads = tape.gradient(target = tr_loss, sources = model.variables) \n",
    "        opt.apply_gradients(grads_and_vars=zip(grads, model.variables)) # gradinet descent\n",
    "        avg_tr_loss += tr_loss\n",
    "        tr_step += 1\n",
    "    else:\n",
    "        avg_tr_loss /= tr_step\n",
    "        tr_loss_hist.append(avg_tr_loss)\n",
    "        \n",
    "    if (epoch +1 ) % 5 == 0:\n",
    "        print(epoch, avg_tr_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9176bbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-04T08:08:04.038289Z",
     "start_time": "2022-01-04T08:08:03.110049Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc : 100.00%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1c46bea4e80>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "yhat = model.predict(x_data)\n",
    "yhat = np.argmax(yhat, axis = -1)\n",
    "print('acc : {:.2%}'.format(np.mean(yhat == y_data)))\n",
    "plt.plot(tr_loss_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d41089",
   "metadata": {},
   "source": [
    "### many to one stacking\n",
    "\n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fbq2eGC%2FbtqDx5KIkkV%2FuLvZu6kmXJfVttnV3Axibk%2Fimg.png\">\n",
    "\n",
    "- stacking: RNN을 여러개 쌓는 것. multilayer RNN, stacked RNN\n",
    "- stacking의 이점\n",
    "    - input과 가까운 부분에서는 edge와 같은 global한 feature을 찾을 수 있음. NLP에선 Syntatic한 정보를 가짐.\n",
    "    - output과 가까운 부분에서는 좀더 abstract한 feature를 찾을 수 있음. NLP에선 Semantic한 정보를 가짐\n",
    "    - 좋은 성능을 만든다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b37a3aec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-04T10:08:21.673156Z",
     "start_time": "2022-01-04T10:08:21.666153Z"
    }
   },
   "outputs": [],
   "source": [
    "# example data\n",
    "sentences = [['I', 'feel', 'hungry'],\n",
    "     ['tensorflow', 'is', 'very', 'difficult'],\n",
    "     ['tensorflow', 'is', 'a', 'framework', 'for', 'deep', 'learning'],\n",
    "     ['tensorflow', 'is', 'very', 'fast', 'changing']]\n",
    "pos = [['pronoun', 'verb', 'adjective'],\n",
    "     ['noun', 'verb', 'adverb', 'adjective'],\n",
    "     ['noun', 'verb', 'determiner', 'noun', 'preposition', 'adjective', 'noun'],\n",
    "     ['noun', 'verb', 'adverb', 'adjective', 'verb']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6f51b0b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-04T10:08:21.883290Z",
     "start_time": "2022-01-04T10:08:21.877288Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<pad>': 0, 'I': 1, 'a': 2, 'changing': 3, 'deep': 4, 'difficult': 5, 'fast': 6, 'feel': 7, 'for': 8, 'framework': 9, 'hungry': 10, 'is': 11, 'learning': 12, 'tensorflow': 13, 'very': 14}\n",
      "{0: '<pad>', 1: 'I', 2: 'a', 3: 'changing', 4: 'deep', 5: 'difficult', 6: 'fast', 7: 'feel', 8: 'for', 9: 'framework', 10: 'hungry', 11: 'is', 12: 'learning', 13: 'tensorflow', 14: 'very'}\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "# creating a token dictionary for word\n",
    "word_list = sum(sentences, [])\n",
    "word_list = sorted(set(word_list))\n",
    "word_list = ['<pad>'] + word_list\n",
    "word2idx = {word : idx for idx, word in enumerate(word_list)}\n",
    "idx2word = {idx : word for idx, word in enumerate(word_list)}\n",
    "\n",
    "print(word2idx)\n",
    "print(idx2word)\n",
    "print(len(idx2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8fb51f27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-04T10:08:22.063329Z",
     "start_time": "2022-01-04T10:08:22.052327Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<pad>': 0, 'adjective': 1, 'adverb': 2, 'determiner': 3, 'noun': 4, 'preposition': 5, 'pronoun': 6, 'verb': 7}\n",
      "{0: '<pad>', 1: 'adjective', 2: 'adverb', 3: 'determiner', 4: 'noun', 5: 'preposition', 6: 'pronoun', 7: 'verb'}\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "# creating a token dictionary for part of speech\n",
    "pos_list = sum(pos, [])\n",
    "pos_list = sorted(set(pos_list))\n",
    "pos_list = ['<pad>'] + pos_list\n",
    "pos2idx = {pos : idx for idx, pos in enumerate(pos_list)}\n",
    "idx2pos = {idx : pos for idx, pos in enumerate(pos_list)}\n",
    "\n",
    "print(pos2idx)\n",
    "print(idx2pos)\n",
    "print(len(pos2idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "90e2a3b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-04T10:08:22.273414Z",
     "start_time": "2022-01-04T10:08:22.268404Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  7 10  0  0  0  0  0  0  0]\n",
      " [13 11 14  5  0  0  0  0  0  0]\n",
      " [13 11  2  9  8  4 12  0  0  0]\n",
      " [13 11 14  6  3  0  0  0  0  0]]\n",
      "[3, 4, 7, 5]\n",
      "[[6 7 1 0 0 0 0 0 0 0]\n",
      " [4 7 2 1 0 0 0 0 0 0]\n",
      " [4 7 3 4 5 1 4 0 0 0]\n",
      " [4 7 2 1 7 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# padding the sequence of indices\n",
    "max_sequence = 10\n",
    "x_data = pad_sequences(sequences = x_data, maxlen = max_sequence,\n",
    "                       padding = 'post', truncating = 'post')\n",
    "\n",
    "# checking data\n",
    "print(x_data)\n",
    "print(x_data_len)\n",
    "print(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "49d4151f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-04T10:08:22.663654Z",
     "start_time": "2022-01-04T10:08:22.478593Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 10, 11)            121       \n",
      "_________________________________________________________________\n",
      "simple_rnn_7 (SimpleRNN)     (None, 10, 10)            220       \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 10, 10)            0         \n",
      "_________________________________________________________________\n",
      "simple_rnn_8 (SimpleRNN)     (None, 10)                210       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 573\n",
      "Trainable params: 452\n",
      "Non-trainable params: 121\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## rnn을 2개 사용하는 stacked RNN구조를 만들어보자.\n",
    "\n",
    "num_classes = 2\n",
    "hidden_dims = [10,10]\n",
    "\n",
    "input_dim = len(char2idx)\n",
    "output_dim = len(char2idx)\n",
    "one_hot = np.eye(len(char2idx))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(input_dim = input_dim, output_dim = output_dim,\n",
    "                          trainable = False, mask_zero = True, input_length = max_sequence,\n",
    "                           embeddings_initializer = keras.initializers.Constant(one_hot)))\n",
    "model.add(layers.SimpleRNN(units = hidden_dims[0], return_sequences = True))\n",
    "model.add(layers.TimeDistributed(layers.Dropout(rate = .2)))\n",
    "model.add(layers.SimpleRNN(units = hidden_dims[1]))\n",
    "model.add(layers.Dropout(rate = .2))\n",
    "model.add(layers.Dense(units = num_classes))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "39ed6674",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-04T10:08:22.768658Z",
     "start_time": "2022-01-04T10:08:22.756656Z"
    }
   },
   "outputs": [],
   "source": [
    "def loss_fun(model, x, y, training): # training drop out을 training 단계에서만 사용한다는 의미\n",
    "    return tf.compat.v1.losses.sparse_softmax_cross_entropy(labels = y, logits = model(x, training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "086e17b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-04T10:08:23.099146Z",
     "start_time": "2022-01-04T10:08:23.093139Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((None, 10), (None, 10)), types: (tf.int32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "lr = .01\n",
    "epochs = 30\n",
    "batch_size = 2\n",
    "opt = tf.keras.optimizers.Adam(lr)\n",
    "\n",
    "# generating data pipeline\n",
    "tr_dataset = tf.data.Dataset.from_tensor_slices((x_data, y_data))\n",
    "tr_dataset = tr_dataset.shuffle(buffer_size=4)\n",
    "tr_dataset = tr_dataset.batch(batch_size=batch_size)\n",
    "\n",
    "print(tr_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "86488e4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-04T10:08:23.521757Z",
     "start_time": "2022-01-04T10:08:23.491752Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "loss_fn() got an unexpected keyword argument 'training'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_30600/351618626.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mx_mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtr_dataset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m             \u001b[0mtr_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx_mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtr_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msources\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: loss_fn() got an unexpected keyword argument 'training'"
     ]
    }
   ],
   "source": [
    "tr_loss_hist = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    avg_tr_loss = 0\n",
    "    tr_step = 0\n",
    "    \n",
    "    for x_mb, y_mb in tr_dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            tr_loss = loss_fn(model, x=x_mb, y=y_mb, training=True)\n",
    "        grads = tape.gradient(target = tr_loss, sources = model.variables)\n",
    "        opt.apply_gradients(grads_and_vars= zip(grads, model.variables))\n",
    "        avg_tr_loss += tr_loss\n",
    "        tr_step += 1\n",
    "    else:\n",
    "        avg_tr_loss /= tr_step\n",
    "        tr_loss_hist.append(avg_tr_loss)\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print('epoch : {:3}, tr_loss : {:.3f}'.format(epoch + 1, avg_tr_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37147d30",
   "metadata": {},
   "source": [
    "## many to many \n",
    "\n",
    "- 형태소 분석기 모델링\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/700/1*_p0fvCt7noN7j6bXQcZjtQ.png\">\n",
    "\n",
    "\n",
    "- many to many: 각각의 token에 대해서 모두 ouput을 내주는 것.\n",
    "\n",
    "### 계산 방법\n",
    "\n",
    "1. sentence를 tokenization을 하고, embedding하여, numeric vector로 만듦. \n",
    "2. rnn layer에 돌려서, 토큰 마다의 ouput을 낸다. \n",
    "3. ouput을 결과와 비교해서, token 마다의 loss를 구함.\n",
    "4. 모든 token에 대해 계산된 loss를 평균을 구해서, sequence loss을 만듦.\n",
    "5. sequence loss를 통해서 backpropagation을 한다.\n",
    "\n",
    "### 단점\n",
    "\n",
    "- 모든 문장의 길이가 동일 할 수 없기 때문에 padding에서 문제가 생김.\n",
    "- masking을 해야함.\n",
    "- masking: pad에 대해서는 loss를 계산하지 않는다는 의미."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c3f8629b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-04T08:12:48.657378Z",
     "start_time": "2022-01-04T08:12:48.651379Z"
    }
   },
   "outputs": [],
   "source": [
    "# example data\n",
    "sentences = [['I', 'feel', 'hungry'],\n",
    "     ['tensorflow', 'is', 'very', 'difficult'],\n",
    "     ['tensorflow', 'is', 'a', 'framework', 'for', 'deep', 'learning'],\n",
    "     ['tensorflow', 'is', 'very', 'fast', 'changing']]\n",
    "pos = [['pronoun', 'verb', 'adjective'],\n",
    "     ['noun', 'verb', 'adverb', 'adjective'],\n",
    "     ['noun', 'verb', 'determiner', 'noun', 'preposition', 'adjective', 'noun'],\n",
    "     ['noun', 'verb', 'adverb', 'adjective', 'verb']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3817f887",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-04T08:12:49.002648Z",
     "start_time": "2022-01-04T08:12:48.989645Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<pad>': 0, 'I': 1, 'a': 2, 'changing': 3, 'deep': 4, 'difficult': 5, 'fast': 6, 'feel': 7, 'for': 8, 'framework': 9, 'hungry': 10, 'is': 11, 'learning': 12, 'tensorflow': 13, 'very': 14}\n",
      "{0: '<pad>', 1: 'I', 2: 'a', 3: 'changing', 4: 'deep', 5: 'difficult', 6: 'fast', 7: 'feel', 8: 'for', 9: 'framework', 10: 'hungry', 11: 'is', 12: 'learning', 13: 'tensorflow', 14: 'very'}\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "# creating a token dictionary for word\n",
    "word_list = sum(sentences, [])\n",
    "word_list = sorted(set(word_list))\n",
    "word_list = ['<pad>'] + word_list\n",
    "word2idx = {word : idx for idx, word in enumerate(word_list)}\n",
    "idx2word = {idx : word for idx, word in enumerate(word_list)}\n",
    "\n",
    "print(word2idx)\n",
    "print(idx2word)\n",
    "print(len(idx2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "13a2d0c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-04T08:12:49.273189Z",
     "start_time": "2022-01-04T08:12:49.264186Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  7 10  0  0  0  0  0  0  0]\n",
      " [13 11 14  5  0  0  0  0  0  0]\n",
      " [13 11  2  9  8  4 12  0  0  0]\n",
      " [13 11 14  6  3  0  0  0  0  0]] [3, 4, 7, 5]\n",
      "[[1. 1. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0. 0. 0. 0.]]\n",
      "[[6 7 1 0 0 0 0 0 0 0]\n",
      " [4 7 2 1 0 0 0 0 0 0]\n",
      " [4 7 3 4 5 1 4 0 0 0]\n",
      " [4 7 2 1 7 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# creating a token dictionary for part of speech\n",
    "# converting sequence of tokens to sequence of indices\n",
    "max_sequence = 10\n",
    "x_data = list(map(lambda sentence : [word2idx.get(token) for token in sentence], sentences))\n",
    "y_data = list(map(lambda sentence : [pos2idx.get(token) for token in sentence], pos))\n",
    "\n",
    "# padding the sequence of indices\n",
    "x_data = pad_sequences(sequences = x_data, maxlen = max_sequence, padding='post')\n",
    "x_data_mask = ((x_data != 0) * 1).astype(np.float32)\n",
    "x_data_len = list(map(lambda sentence : len(sentence), sentences))\n",
    "\n",
    "y_data = pad_sequences(sequences = y_data, maxlen = max_sequence, padding='post')\n",
    "\n",
    "# checking data\n",
    "print(x_data, x_data_len)\n",
    "print(x_data_mask)\n",
    "print(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5f58d197",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-04T08:12:49.997261Z",
     "start_time": "2022-01-04T08:12:49.890239Z"
    }
   },
   "outputs": [],
   "source": [
    "# creating rnn for \"many to many\" sequence tagging\n",
    "num_classes = len(pos2idx)\n",
    "hidden_dim = 10\n",
    "\n",
    "input_dim = len(word2idx)\n",
    "output_dim = len(word2idx)\n",
    "one_hot = np.eye(len(word2idx))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(input_dim=input_dim, output_dim=output_dim, mask_zero=True,## masking을 사용함\n",
    "                           trainable=False, input_length=max_sequence,\n",
    "                           embeddings_initializer=keras.initializers.Constant(one_hot)))\n",
    "                                                    ## token을 onehot vector로 만든다.\n",
    "model.add(layers.SimpleRNN(units=hidden_dim, return_sequences=True))## rnn이 있는 모든 토큰에 대해서 출력\n",
    "model.add(layers.TimeDistributed(layers.Dense(units=num_classes))) ## 매번 품사를 찾음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ae242927",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-04T08:13:19.471534Z",
     "start_time": "2022-01-04T08:13:19.464525Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 10, 15)            225       \n",
      "_________________________________________________________________\n",
      "simple_rnn_6 (SimpleRNN)     (None, 10, 10)            260       \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 10, 8)             88        \n",
      "=================================================================\n",
      "Total params: 573\n",
      "Trainable params: 348\n",
      "Non-trainable params: 225\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb0966c",
   "metadata": {},
   "source": [
    "### loss function 계산 방법\n",
    "\n",
    "- x의 길이만큼의 masking vector 형성, 하지만 유효한 길이는 실제 길이\n",
    "- rnn이 sequence데이터의 각각의 값이 정답에 들어갈 확률을 계산\n",
    "- loss function은 예상한 값과 실제 값의 차이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "26767861",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-04T10:07:54.334116Z",
     "start_time": "2022-01-04T10:07:54.328110Z"
    }
   },
   "outputs": [],
   "source": [
    "def loss_fn(model, x, y, x_len, max_sequence):\n",
    "    masking = tf.sequence_mask(x_len, maxlen=max_sequence, dtype=tf.float32)\n",
    "    valid_time_step = tf.cast(x_len,dtype=tf.float32)\n",
    "    sequence_loss = tf.compat.v1.losses.sparse_softmax_cross_entropy(labels=y, logits=model(x),\n",
    "                                                           reduction='none') * masking\n",
    "    sequence_loss = tf.reduce_sum(sequence_loss, axis=-1) / valid_time_step\n",
    "    sequence_loss = tf.reduce_mean(sequence_loss)\n",
    "    return sequence_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b58e8e5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-04T10:07:54.784995Z",
     "start_time": "2022-01-04T10:07:54.777997Z"
    }
   },
   "outputs": [],
   "source": [
    "# creating and optimizer\n",
    "lr = 0.1\n",
    "epochs = 30\n",
    "batch_size = 2 \n",
    "opt = tf.keras.optimizers.Adam(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9f408946",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-04T10:07:55.100663Z",
     "start_time": "2022-01-04T10:07:55.094662Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((None, 10), (None, 10), (None,)), types: (tf.int32, tf.int32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "# generating data pipeline\n",
    "tr_dataset = tf.data.Dataset.from_tensor_slices((x_data, y_data, x_data_len))\n",
    "tr_dataset = tr_dataset.shuffle(buffer_size=4)\n",
    "tr_dataset = tr_dataset.batch(batch_size = 2)\n",
    "\n",
    "print(tr_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "eadd85b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-04T10:07:57.666551Z",
     "start_time": "2022-01-04T10:07:55.369262Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :   5, tr_loss : 0.092\n",
      "epoch :  10, tr_loss : 0.004\n",
      "epoch :  15, tr_loss : 0.001\n",
      "epoch :  20, tr_loss : 0.000\n",
      "epoch :  25, tr_loss : 0.000\n",
      "epoch :  30, tr_loss : 0.000\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "tr_loss_hist = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    avg_tr_loss = 0\n",
    "    tr_step = 0\n",
    "    \n",
    "    for x_mb, y_mb, x_mb_len in tr_dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            tr_loss = loss_fn(model, x=x_mb, y=y_mb, x_len=x_mb_len, max_sequence=max_sequence)\n",
    "        grads = tape.gradient(target=tr_loss, sources=model.variables)\n",
    "        opt.apply_gradients(grads_and_vars=zip(grads, model.variables))\n",
    "        avg_tr_loss += tr_loss\n",
    "        tr_step += 1\n",
    "    else:\n",
    "        avg_tr_loss /= tr_step\n",
    "        tr_loss_hist.append(avg_tr_loss)\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print('epoch : {:3}, tr_loss : {:.3f}'.format(epoch + 1, avg_tr_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0fe5c116",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-04T10:09:22.633005Z",
     "start_time": "2022-01-04T10:09:22.617996Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.32597417, -0.43574494],\n",
       "       [ 0.15637216, -0.0997463 ],\n",
       "       [ 0.556416  ,  0.57665074],\n",
       "       [ 0.47114515, -1.1276963 ]], dtype=float32)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "caad4158",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-04T10:08:51.962232Z",
     "start_time": "2022-01-04T10:08:51.590142Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (4,) (4,10) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_30600/229364131.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0myhat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0myhat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mx_data_mask\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0midx2pos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melm\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0melm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0myhat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m120\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (4,) (4,10) "
     ]
    }
   ],
   "source": [
    "yhat = model.predict(x_data)\n",
    "yhat = np.argmax(yhat, axis=-1) * x_data_mask\n",
    "\n",
    "pprint(list(map(lambda row : [idx2pos.get(elm) for elm in row],yhat.astype(np.int32).tolist())), width = 120)\n",
    "pprint(pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bec53e",
   "metadata": {},
   "source": [
    "# bidirectional \n",
    "\n",
    "- 기존에는 단 방향으로 RNN을 사용.\n",
    "- 문제: 정보의 불균형 문제가 일어남. \n",
    "    ex) 첫번째 은닉층은 output은 x_1의 영향만 받지만, 2번재 token에서는 2개의 정보를 통해 ouput을 배출.\n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fdg57jy%2Fbtq4DhreQuh%2F5UwFkiniBWsMYChavAKRG0%2Fimg.png\">    \n",
    "    \n",
    "- 해결: foward, backward RNN을 두어 문제를 해결한다. \n",
    "    - 은닉층을 2개 두어서, 첫번째 층은 순서대로 학습한다. 다음 층은 역순으로 학습하여, 첫번째 셀이 유의미한 정보값을 가지도록 함.\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fd1802",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f23b9215",
   "metadata": {},
   "source": [
    "# seq2seq\n",
    "\n",
    "## overview\n",
    "\n",
    "- 문제의식: 책봇이나 번역처럼 input 전체의 값이 ouput에 필요한 경엔, input 값을 다보고, ouput을 넣는게 모델의 성능에 더 좋지 않을까?\n",
    "\n",
    "### encoder - decoder\n",
    "- encoder: 입력의 정보를 가지고 있는 벡터를 만드는 layer들\n",
    "- decoder: encoder을 커쳐온 마지막 벡터를 입력값으로 ouput 값을 만드는 layer. 각 step에서 output이 하나씩 나온다. \n",
    "- 학습 과정: encoder layer -> context vector -> decoder\n",
    "\n",
    "- 문제점: encoder 부분이 너무 길어지면 벡터에 충분한 정보를 담기 어려워지는 경향이 생긴다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5aba8b25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-04T10:48:07.068887Z",
     "start_time": "2022-01-04T10:48:06.118343Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "# Import TensorFlow >= 1.10 and enable eager execution\n",
    "import tensorflow as tf\n",
    "tf.executing_eagerly()\n",
    "\n",
    "from matplotlib import font_manager, rc\n",
    "\n",
    "rc('font', family='AppleGothic') #for mac\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e1beb592",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-04T10:48:45.964601Z",
     "start_time": "2022-01-04T10:48:45.958601Z"
    }
   },
   "outputs": [],
   "source": [
    "sources = [['I', 'feel', 'hungry'],\n",
    "     ['tensorflow', 'is', 'very', 'difficult'],\n",
    "     ['tensorflow', 'is', 'a', 'framework', 'for', 'deep', 'learning'],\n",
    "     ['tensorflow', 'is', 'very', 'fast', 'changing']]\n",
    "targets = [['나는', '배가', '고프다'],\n",
    "           ['텐서플로우는', '매우', '어렵다'],\n",
    "           ['텐서플로우는', '딥러닝을', '위한', '프레임워크이다'],\n",
    "           ['텐서플로우는', '매우', '빠르게', '변화한다']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6b5a9926",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-04T10:48:51.726556Z",
     "start_time": "2022-01-04T10:48:51.717556Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<pad>': 0,\n",
      " 'I': 1,\n",
      " 'a': 2,\n",
      " 'changing': 3,\n",
      " 'deep': 4,\n",
      " 'difficult': 5,\n",
      " 'fast': 6,\n",
      " 'feel': 7,\n",
      " 'for': 8,\n",
      " 'framework': 9,\n",
      " 'hungry': 10,\n",
      " 'is': 11,\n",
      " 'learning': 12,\n",
      " 'tensorflow': 13,\n",
      " 'very': 14}\n"
     ]
    }
   ],
   "source": [
    "# vocabulary for sources\n",
    "s_vocab = list(set(sum(sources, [])))\n",
    "s_vocab.sort()\n",
    "s_vocab = ['<pad>'] + s_vocab\n",
    "source2idx = {word : idx for idx, word in enumerate(s_vocab)}\n",
    "idx2source = {idx : word for idx, word in enumerate(s_vocab)}\n",
    "\n",
    "pprint(source2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8b1709e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-04T10:48:57.149392Z",
     "start_time": "2022-01-04T10:48:57.131380Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<bos>': 1,\n",
      " '<eos>': 2,\n",
      " '<pad>': 0,\n",
      " '고프다': 3,\n",
      " '나는': 4,\n",
      " '딥러닝을': 5,\n",
      " '매우': 6,\n",
      " '배가': 7,\n",
      " '변화한다': 8,\n",
      " '빠르게': 9,\n",
      " '어렵다': 10,\n",
      " '위한': 11,\n",
      " '텐서플로우는': 12,\n",
      " '프레임워크이다': 13}\n"
     ]
    }
   ],
   "source": [
    "# vocabulary for targets\n",
    "t_vocab = list(set(sum(targets, [])))\n",
    "t_vocab.sort()\n",
    "t_vocab = ['<pad>', '<bos>', '<eos>'] + t_vocab\n",
    "target2idx = {word : idx for idx, word in enumerate(t_vocab)}\n",
    "idx2target = {idx : word for idx, word in enumerate(t_vocab)}\n",
    "\n",
    "pprint(target2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912f9085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sequences, max_len, dic, mode = 'source'):\n",
    "    assert mode in ['source', 'target'], 'source나 target을 선택해주세요.'\n",
    "    \n",
    "    if mode == 'source':\n",
    "        s_input = list(map(lambda sentence : [dic.get(token) for token in sentence], sequences))\n",
    "        s_len = list(map(lambda sentence : len(sentence), s_input))\n",
    "        s_input = pad_sentences(sequences = s_input, maxlen = max_len, padding = 'post', trucating = 'post')\n",
    "        return s_len, s_input\n",
    "    \n",
    "    elif mode == 'target':               ## 문장의 처음과 끝에 시작과 끝 값을 넣어준다.\n",
    "        t_input = list(map(lambda sentence : ['<bos>'] + sentence + ['<eos>'], sequences))\n",
    "        t_input = list(map(lambda sentence : []))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac1c006",
   "metadata": {},
   "source": [
    "## techer forcing\n",
    "\n",
    "- 기존 RNN에서는 $x_{t2}$의 입력값으로 $x_{t0}$과 $\\hat{y_{t1}}$를 사용함. 하지만 $\\hat{y_{t1}}$이 true ${x_{t1}}$와 다르다면, 이후의 예측값에 악영향을 미침. techer forcing은 이런 문제점을 보완하기 위해서 $\\hat{y_{t1}}$이 아닌 true ${x_{t1}}$을 입력값으로 사용한다.\n",
    "- 잘못 예측 하더라도, 예측값에 영향을 미치지 않도록 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008575de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c78e1f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4beabb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb43385f",
   "metadata": {},
   "source": [
    "## seq2seq attention\n",
    "\n",
    "- attention이란?\n",
    "    - 핵심 단어를 문장에서 골라서, 더 중요한 정보를 표시하는 방법. \n",
    "    - decoder에서 단어를 예측하는 step마다 encoder에서의 출력 문장을 다시 한번 확인하자. \n",
    "    - 하지만 중요한 단어에 큰 가중치를 주자. \n",
    "- seq2seq의 문제점: 단어의 길이가 커지면 문제가 생긴다. \n",
    "    - 예를 들어서 encoder의 input이 너무 길면, 필요한 정보를 context에서 가지지 못함.\n",
    "- seq2seq와 차이점: 이전 encoder의 step의 영향을 모두 사용함\n",
    "- 구현 방법: context vector에서 \n",
    "현재 step에서 가장 중요한 영향을 미친 \n",
    "attention 구조에서 매 step을 확\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "tacademy",
   "language": "python",
   "name": "tacademy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a819d95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T06:40:21.673293Z",
     "start_time": "2021-12-27T06:40:20.052966Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11e10458",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T06:40:21.688297Z",
     "start_time": "2021-12-27T06:40:21.675294Z"
    }
   },
   "outputs": [],
   "source": [
    "x_data = np.array( [[0,0],[0,1],[1,0],[1,1]] )\n",
    "y_data = np.array( [[0],[1],[1],[0] ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5478f451",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T06:40:21.703300Z",
     "start_time": "2021-12-27T06:40:21.690298Z"
    }
   },
   "outputs": [],
   "source": [
    "model_logi = LogisticRegression(max_iter = 1000, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f1c1df1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T06:40:21.733799Z",
     "start_time": "2021-12-27T06:40:21.704303Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\13a71\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000, verbose=1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_logi.fit(x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c292bf0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T06:40:21.748820Z",
     "start_time": "2021-12-27T06:40:21.735805Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_logi.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b81ff9b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T06:40:21.763802Z",
     "start_time": "2021-12-27T06:40:21.749800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_logi.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c1a3509",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T06:40:21.778805Z",
     "start_time": "2021-12-27T06:40:21.765811Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_logi.predict(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74f2a263",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T06:40:21.793808Z",
     "start_time": "2021-12-27T06:40:21.780806Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_data,  model_logi.predict(x_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12f228d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T06:40:21.808813Z",
     "start_time": "2021-12-27T06:40:21.795811Z"
    }
   },
   "outputs": [],
   "source": [
    "model_mlp = MLPClassifier(verbose = True, hidden_layer_sizes= (100,), max_iter= 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9733c6d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T06:40:22.153782Z",
     "start_time": "2021-12-27T06:40:21.810812Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\13a71\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.68746950\n",
      "Iteration 2, loss = 0.68598888\n",
      "Iteration 3, loss = 0.68455925\n",
      "Iteration 4, loss = 0.68316283\n",
      "Iteration 5, loss = 0.68181282\n",
      "Iteration 6, loss = 0.68045657\n",
      "Iteration 7, loss = 0.67911315\n",
      "Iteration 8, loss = 0.67788304\n",
      "Iteration 9, loss = 0.67660360\n",
      "Iteration 10, loss = 0.67537142\n",
      "Iteration 11, loss = 0.67417701\n",
      "Iteration 12, loss = 0.67298604\n",
      "Iteration 13, loss = 0.67187290\n",
      "Iteration 14, loss = 0.67075937\n",
      "Iteration 15, loss = 0.66967082\n",
      "Iteration 16, loss = 0.66860628\n",
      "Iteration 17, loss = 0.66753631\n",
      "Iteration 18, loss = 0.66645884\n",
      "Iteration 19, loss = 0.66538722\n",
      "Iteration 20, loss = 0.66436662\n",
      "Iteration 21, loss = 0.66337637\n",
      "Iteration 22, loss = 0.66239126\n",
      "Iteration 23, loss = 0.66138250\n",
      "Iteration 24, loss = 0.66036151\n",
      "Iteration 25, loss = 0.65933905\n",
      "Iteration 26, loss = 0.65830272\n",
      "Iteration 27, loss = 0.65727767\n",
      "Iteration 28, loss = 0.65624483\n",
      "Iteration 29, loss = 0.65522089\n",
      "Iteration 30, loss = 0.65420034\n",
      "Iteration 31, loss = 0.65315860\n",
      "Iteration 32, loss = 0.65210248\n",
      "Iteration 33, loss = 0.65105478\n",
      "Iteration 34, loss = 0.64999566\n",
      "Iteration 35, loss = 0.64895277\n",
      "Iteration 36, loss = 0.64792174\n",
      "Iteration 37, loss = 0.64685224\n",
      "Iteration 38, loss = 0.64576894\n",
      "Iteration 39, loss = 0.64471268\n",
      "Iteration 40, loss = 0.64365257\n",
      "Iteration 41, loss = 0.64257983\n",
      "Iteration 42, loss = 0.64149185\n",
      "Iteration 43, loss = 0.64040781\n",
      "Iteration 44, loss = 0.63930508\n",
      "Iteration 45, loss = 0.63820708\n",
      "Iteration 46, loss = 0.63710320\n",
      "Iteration 47, loss = 0.63598392\n",
      "Iteration 48, loss = 0.63489478\n",
      "Iteration 49, loss = 0.63375123\n",
      "Iteration 50, loss = 0.63258280\n",
      "Iteration 51, loss = 0.63144582\n",
      "Iteration 52, loss = 0.63031834\n",
      "Iteration 53, loss = 0.62915163\n",
      "Iteration 54, loss = 0.62796793\n",
      "Iteration 55, loss = 0.62678189\n",
      "Iteration 56, loss = 0.62559431\n",
      "Iteration 57, loss = 0.62440082\n",
      "Iteration 58, loss = 0.62320000\n",
      "Iteration 59, loss = 0.62199644\n",
      "Iteration 60, loss = 0.62079965\n",
      "Iteration 61, loss = 0.61962272\n",
      "Iteration 62, loss = 0.61840828\n",
      "Iteration 63, loss = 0.61719282\n",
      "Iteration 64, loss = 0.61594146\n",
      "Iteration 65, loss = 0.61469601\n",
      "Iteration 66, loss = 0.61346991\n",
      "Iteration 67, loss = 0.61225969\n",
      "Iteration 68, loss = 0.61103233\n",
      "Iteration 69, loss = 0.60978237\n",
      "Iteration 70, loss = 0.60850542\n",
      "Iteration 71, loss = 0.60723425\n",
      "Iteration 72, loss = 0.60598901\n",
      "Iteration 73, loss = 0.60473136\n",
      "Iteration 74, loss = 0.60347271\n",
      "Iteration 75, loss = 0.60224200\n",
      "Iteration 76, loss = 0.60098199\n",
      "Iteration 77, loss = 0.59969760\n",
      "Iteration 78, loss = 0.59838691\n",
      "Iteration 79, loss = 0.59704899\n",
      "Iteration 80, loss = 0.59574676\n",
      "Iteration 81, loss = 0.59446632\n",
      "Iteration 82, loss = 0.59318127\n",
      "Iteration 83, loss = 0.59188918\n",
      "Iteration 84, loss = 0.59057972\n",
      "Iteration 85, loss = 0.58926340\n",
      "Iteration 86, loss = 0.58794768\n",
      "Iteration 87, loss = 0.58661727\n",
      "Iteration 88, loss = 0.58525878\n",
      "Iteration 89, loss = 0.58390842\n",
      "Iteration 90, loss = 0.58252066\n",
      "Iteration 91, loss = 0.58115963\n",
      "Iteration 92, loss = 0.57978172\n",
      "Iteration 93, loss = 0.57840606\n",
      "Iteration 94, loss = 0.57698921\n",
      "Iteration 95, loss = 0.57557899\n",
      "Iteration 96, loss = 0.57416838\n",
      "Iteration 97, loss = 0.57272424\n",
      "Iteration 98, loss = 0.57131018\n",
      "Iteration 99, loss = 0.56987239\n",
      "Iteration 100, loss = 0.56840488\n",
      "Iteration 101, loss = 0.56694953\n",
      "Iteration 102, loss = 0.56547205\n",
      "Iteration 103, loss = 0.56399853\n",
      "Iteration 104, loss = 0.56247141\n",
      "Iteration 105, loss = 0.56097116\n",
      "Iteration 106, loss = 0.55946627\n",
      "Iteration 107, loss = 0.55794659\n",
      "Iteration 108, loss = 0.55642396\n",
      "Iteration 109, loss = 0.55489486\n",
      "Iteration 110, loss = 0.55336725\n",
      "Iteration 111, loss = 0.55180223\n",
      "Iteration 112, loss = 0.55024219\n",
      "Iteration 113, loss = 0.54866331\n",
      "Iteration 114, loss = 0.54706121\n",
      "Iteration 115, loss = 0.54548639\n",
      "Iteration 116, loss = 0.54390015\n",
      "Iteration 117, loss = 0.54232792\n",
      "Iteration 118, loss = 0.54068661\n",
      "Iteration 119, loss = 0.53904379\n",
      "Iteration 120, loss = 0.53741929\n",
      "Iteration 121, loss = 0.53580044\n",
      "Iteration 122, loss = 0.53414993\n",
      "Iteration 123, loss = 0.53249550\n",
      "Iteration 124, loss = 0.53083939\n",
      "Iteration 125, loss = 0.52917102\n",
      "Iteration 126, loss = 0.52746830\n",
      "Iteration 127, loss = 0.52581972\n",
      "Iteration 128, loss = 0.52410611\n",
      "Iteration 129, loss = 0.52244331\n",
      "Iteration 130, loss = 0.52072719\n",
      "Iteration 131, loss = 0.51904274\n",
      "Iteration 132, loss = 0.51735062\n",
      "Iteration 133, loss = 0.51560673\n",
      "Iteration 134, loss = 0.51385637\n",
      "Iteration 135, loss = 0.51214587\n",
      "Iteration 136, loss = 0.51039834\n",
      "Iteration 137, loss = 0.50868390\n",
      "Iteration 138, loss = 0.50692737\n",
      "Iteration 139, loss = 0.50514196\n",
      "Iteration 140, loss = 0.50341699\n",
      "Iteration 141, loss = 0.50171192\n",
      "Iteration 142, loss = 0.49993316\n",
      "Iteration 143, loss = 0.49815706\n",
      "Iteration 144, loss = 0.49635570\n",
      "Iteration 145, loss = 0.49459457\n",
      "Iteration 146, loss = 0.49283874\n",
      "Iteration 147, loss = 0.49107513\n",
      "Iteration 148, loss = 0.48927887\n",
      "Iteration 149, loss = 0.48750988\n",
      "Iteration 150, loss = 0.48571461\n",
      "Iteration 151, loss = 0.48390554\n",
      "Iteration 152, loss = 0.48210372\n",
      "Iteration 153, loss = 0.48030896\n",
      "Iteration 154, loss = 0.47852786\n",
      "Iteration 155, loss = 0.47676190\n",
      "Iteration 156, loss = 0.47497287\n",
      "Iteration 157, loss = 0.47315509\n",
      "Iteration 158, loss = 0.47137675\n",
      "Iteration 159, loss = 0.46959779\n",
      "Iteration 160, loss = 0.46784121\n",
      "Iteration 161, loss = 0.46607684\n",
      "Iteration 162, loss = 0.46429706\n",
      "Iteration 163, loss = 0.46250583\n",
      "Iteration 164, loss = 0.46074296\n",
      "Iteration 165, loss = 0.45894758\n",
      "Iteration 166, loss = 0.45712878\n",
      "Iteration 167, loss = 0.45538078\n",
      "Iteration 168, loss = 0.45360859\n",
      "Iteration 169, loss = 0.45182990\n",
      "Iteration 170, loss = 0.45003406\n",
      "Iteration 171, loss = 0.44825459\n",
      "Iteration 172, loss = 0.44645667\n",
      "Iteration 173, loss = 0.44467770\n",
      "Iteration 174, loss = 0.44293480\n",
      "Iteration 175, loss = 0.44110802\n",
      "Iteration 176, loss = 0.43928588\n",
      "Iteration 177, loss = 0.43750955\n",
      "Iteration 178, loss = 0.43571260\n",
      "Iteration 179, loss = 0.43394488\n",
      "Iteration 180, loss = 0.43218046\n",
      "Iteration 181, loss = 0.43035989\n",
      "Iteration 182, loss = 0.42858509\n",
      "Iteration 183, loss = 0.42678795\n",
      "Iteration 184, loss = 0.42501255\n",
      "Iteration 185, loss = 0.42322781\n",
      "Iteration 186, loss = 0.42148690\n",
      "Iteration 187, loss = 0.41969393\n",
      "Iteration 188, loss = 0.41789121\n",
      "Iteration 189, loss = 0.41610472\n",
      "Iteration 190, loss = 0.41433021\n",
      "Iteration 191, loss = 0.41254707\n",
      "Iteration 192, loss = 0.41079466\n",
      "Iteration 193, loss = 0.40900716\n",
      "Iteration 194, loss = 0.40721682\n",
      "Iteration 195, loss = 0.40544171\n",
      "Iteration 196, loss = 0.40366621\n",
      "Iteration 197, loss = 0.40187391\n",
      "Iteration 198, loss = 0.40014867\n",
      "Iteration 199, loss = 0.39840473\n",
      "Iteration 200, loss = 0.39662070\n",
      "Iteration 201, loss = 0.39484117\n",
      "Iteration 202, loss = 0.39312755\n",
      "Iteration 203, loss = 0.39134032\n",
      "Iteration 204, loss = 0.38957532\n",
      "Iteration 205, loss = 0.38780579\n",
      "Iteration 206, loss = 0.38609160\n",
      "Iteration 207, loss = 0.38433895\n",
      "Iteration 208, loss = 0.38254601\n",
      "Iteration 209, loss = 0.38081032\n",
      "Iteration 210, loss = 0.37904747\n",
      "Iteration 211, loss = 0.37730891\n",
      "Iteration 212, loss = 0.37553259\n",
      "Iteration 213, loss = 0.37380249\n",
      "Iteration 214, loss = 0.37207971\n",
      "Iteration 215, loss = 0.37034563\n",
      "Iteration 216, loss = 0.36863458\n",
      "Iteration 217, loss = 0.36695371\n",
      "Iteration 218, loss = 0.36524887\n",
      "Iteration 219, loss = 0.36357422\n",
      "Iteration 220, loss = 0.36185341\n",
      "Iteration 221, loss = 0.36011014\n",
      "Iteration 222, loss = 0.35842280\n",
      "Iteration 223, loss = 0.35668580\n",
      "Iteration 224, loss = 0.35498576\n",
      "Iteration 225, loss = 0.35324874\n",
      "Iteration 226, loss = 0.35160100\n",
      "Iteration 227, loss = 0.34991615\n",
      "Iteration 228, loss = 0.34823804\n",
      "Iteration 229, loss = 0.34658858\n",
      "Iteration 230, loss = 0.34491068\n",
      "Iteration 231, loss = 0.34325359\n",
      "Iteration 232, loss = 0.34159874\n",
      "Iteration 233, loss = 0.33992762\n",
      "Iteration 234, loss = 0.33823420\n",
      "Iteration 235, loss = 0.33661117\n",
      "Iteration 236, loss = 0.33497431\n",
      "Iteration 237, loss = 0.33330677\n",
      "Iteration 238, loss = 0.33163498\n",
      "Iteration 239, loss = 0.33004161\n",
      "Iteration 240, loss = 0.32843836\n",
      "Iteration 241, loss = 0.32682722\n",
      "Iteration 242, loss = 0.32524252\n",
      "Iteration 243, loss = 0.32361347\n",
      "Iteration 244, loss = 0.32200599\n",
      "Iteration 245, loss = 0.32043566\n",
      "Iteration 246, loss = 0.31883866\n",
      "Iteration 247, loss = 0.31722602\n",
      "Iteration 248, loss = 0.31563132\n",
      "Iteration 249, loss = 0.31405446\n",
      "Iteration 250, loss = 0.31247930\n",
      "Iteration 251, loss = 0.31090206\n",
      "Iteration 252, loss = 0.30935100\n",
      "Iteration 253, loss = 0.30779102\n",
      "Iteration 254, loss = 0.30618676\n",
      "Iteration 255, loss = 0.30464906\n",
      "Iteration 256, loss = 0.30313889\n",
      "Iteration 257, loss = 0.30159307\n",
      "Iteration 258, loss = 0.30007880\n",
      "Iteration 259, loss = 0.29856211\n",
      "Iteration 260, loss = 0.29703112\n",
      "Iteration 261, loss = 0.29549894\n",
      "Iteration 262, loss = 0.29396523\n",
      "Iteration 263, loss = 0.29246195\n",
      "Iteration 264, loss = 0.29098166\n",
      "Iteration 265, loss = 0.28947971\n",
      "Iteration 266, loss = 0.28797388\n",
      "Iteration 267, loss = 0.28648303\n",
      "Iteration 268, loss = 0.28502603\n",
      "Iteration 269, loss = 0.28357453\n",
      "Iteration 270, loss = 0.28211559\n",
      "Iteration 271, loss = 0.28064026\n",
      "Iteration 272, loss = 0.27918212\n",
      "Iteration 273, loss = 0.27773154\n",
      "Iteration 274, loss = 0.27629277\n",
      "Iteration 275, loss = 0.27483915\n",
      "Iteration 276, loss = 0.27339167\n",
      "Iteration 277, loss = 0.27198147\n",
      "Iteration 278, loss = 0.27055670\n",
      "Iteration 279, loss = 0.26912129\n",
      "Iteration 280, loss = 0.26772731\n",
      "Iteration 281, loss = 0.26631356\n",
      "Iteration 282, loss = 0.26490982\n",
      "Iteration 283, loss = 0.26354296\n",
      "Iteration 284, loss = 0.26214944\n",
      "Iteration 285, loss = 0.26076613\n",
      "Iteration 286, loss = 0.25941075\n",
      "Iteration 287, loss = 0.25806585\n",
      "Iteration 288, loss = 0.25668514\n",
      "Iteration 289, loss = 0.25532041\n",
      "Iteration 290, loss = 0.25399898\n",
      "Iteration 291, loss = 0.25264464\n",
      "Iteration 292, loss = 0.25129150\n",
      "Iteration 293, loss = 0.24998267\n",
      "Iteration 294, loss = 0.24864919\n",
      "Iteration 295, loss = 0.24734908\n",
      "Iteration 296, loss = 0.24604062\n",
      "Iteration 297, loss = 0.24472485\n",
      "Iteration 298, loss = 0.24340686\n",
      "Iteration 299, loss = 0.24203236\n",
      "Iteration 300, loss = 0.24063556\n",
      "Iteration 301, loss = 0.23927976\n",
      "Iteration 302, loss = 0.23795476\n",
      "Iteration 303, loss = 0.23659837\n",
      "Iteration 304, loss = 0.23526617\n",
      "Iteration 305, loss = 0.23392316\n",
      "Iteration 306, loss = 0.23259168\n",
      "Iteration 307, loss = 0.23122547\n",
      "Iteration 308, loss = 0.22988629\n",
      "Iteration 309, loss = 0.22857232\n",
      "Iteration 310, loss = 0.22727492\n",
      "Iteration 311, loss = 0.22593745\n",
      "Iteration 312, loss = 0.22459784\n",
      "Iteration 313, loss = 0.22329888\n",
      "Iteration 314, loss = 0.22199028\n",
      "Iteration 315, loss = 0.22066900\n",
      "Iteration 316, loss = 0.21936672\n",
      "Iteration 317, loss = 0.21806038\n",
      "Iteration 318, loss = 0.21677235\n",
      "Iteration 319, loss = 0.21552842\n",
      "Iteration 320, loss = 0.21428327\n",
      "Iteration 321, loss = 0.21305398\n",
      "Iteration 322, loss = 0.21181531\n",
      "Iteration 323, loss = 0.21059454\n",
      "Iteration 324, loss = 0.20937716\n",
      "Iteration 325, loss = 0.20815048\n",
      "Iteration 326, loss = 0.20693460\n",
      "Iteration 327, loss = 0.20572913\n",
      "Iteration 328, loss = 0.20452189\n",
      "Iteration 329, loss = 0.20330601\n",
      "Iteration 330, loss = 0.20209308\n",
      "Iteration 331, loss = 0.20090998\n",
      "Iteration 332, loss = 0.19974300\n",
      "Iteration 333, loss = 0.19855584\n",
      "Iteration 334, loss = 0.19735835\n",
      "Iteration 335, loss = 0.19617652\n",
      "Iteration 336, loss = 0.19504274\n",
      "Iteration 337, loss = 0.19392554\n",
      "Iteration 338, loss = 0.19276362\n",
      "Iteration 339, loss = 0.19163860\n",
      "Iteration 340, loss = 0.19052449\n",
      "Iteration 341, loss = 0.18938588\n",
      "Iteration 342, loss = 0.18826320\n",
      "Iteration 343, loss = 0.18713827\n",
      "Iteration 344, loss = 0.18606705\n",
      "Iteration 345, loss = 0.18497785\n",
      "Iteration 346, loss = 0.18387930\n",
      "Iteration 347, loss = 0.18280358\n",
      "Iteration 348, loss = 0.18174134\n",
      "Iteration 349, loss = 0.18066987\n",
      "Iteration 350, loss = 0.17962137\n",
      "Iteration 351, loss = 0.17855312\n",
      "Iteration 352, loss = 0.17749943\n",
      "Iteration 353, loss = 0.17644546\n",
      "Iteration 354, loss = 0.17541538\n",
      "Iteration 355, loss = 0.17440442\n",
      "Iteration 356, loss = 0.17337275\n",
      "Iteration 357, loss = 0.17234981\n",
      "Iteration 358, loss = 0.17133888\n",
      "Iteration 359, loss = 0.17032144\n",
      "Iteration 360, loss = 0.16932215\n",
      "Iteration 361, loss = 0.16834107\n",
      "Iteration 362, loss = 0.16736373\n",
      "Iteration 363, loss = 0.16638682\n",
      "Iteration 364, loss = 0.16541062\n",
      "Iteration 365, loss = 0.16443695\n",
      "Iteration 366, loss = 0.16348379\n",
      "Iteration 367, loss = 0.16254856\n",
      "Iteration 368, loss = 0.16159910\n",
      "Iteration 369, loss = 0.16065502\n",
      "Iteration 370, loss = 0.15971414\n",
      "Iteration 371, loss = 0.15878447\n",
      "Iteration 372, loss = 0.15786284\n",
      "Iteration 373, loss = 0.15693816\n",
      "Iteration 374, loss = 0.15603147\n",
      "Iteration 375, loss = 0.15513616\n",
      "Iteration 376, loss = 0.15425881\n",
      "Iteration 377, loss = 0.15336418\n",
      "Iteration 378, loss = 0.15246551\n",
      "Iteration 379, loss = 0.15158003\n",
      "Iteration 380, loss = 0.15072698\n",
      "Iteration 381, loss = 0.14984460\n",
      "Iteration 382, loss = 0.14897340\n",
      "Iteration 383, loss = 0.14812370\n",
      "Iteration 384, loss = 0.14728612\n",
      "Iteration 385, loss = 0.14645129\n",
      "Iteration 386, loss = 0.14561275\n",
      "Iteration 387, loss = 0.14477238\n",
      "Iteration 388, loss = 0.14394067\n",
      "Iteration 389, loss = 0.14311674\n",
      "Iteration 390, loss = 0.14231284\n",
      "Iteration 391, loss = 0.14151492\n",
      "Iteration 392, loss = 0.14073246\n",
      "Iteration 393, loss = 0.13995914\n",
      "Iteration 394, loss = 0.13917462\n",
      "Iteration 395, loss = 0.13838951\n",
      "Iteration 396, loss = 0.13762357\n",
      "Iteration 397, loss = 0.13685620\n",
      "Iteration 398, loss = 0.13606830\n",
      "Iteration 399, loss = 0.13531495\n",
      "Iteration 400, loss = 0.13456642\n",
      "Iteration 401, loss = 0.13379364\n",
      "Iteration 402, loss = 0.13303937\n",
      "Iteration 403, loss = 0.13233179\n",
      "Iteration 404, loss = 0.13161181\n",
      "Iteration 405, loss = 0.13087436\n",
      "Iteration 406, loss = 0.13013584\n",
      "Iteration 407, loss = 0.12940756\n",
      "Iteration 408, loss = 0.12870322\n",
      "Iteration 409, loss = 0.12799990\n",
      "Iteration 410, loss = 0.12729009\n",
      "Iteration 411, loss = 0.12660108\n",
      "Iteration 412, loss = 0.12590610\n",
      "Iteration 413, loss = 0.12520219\n",
      "Iteration 414, loss = 0.12451257\n",
      "Iteration 415, loss = 0.12383757\n",
      "Iteration 416, loss = 0.12316128\n",
      "Iteration 417, loss = 0.12249558\n",
      "Iteration 418, loss = 0.12183472\n",
      "Iteration 419, loss = 0.12116851\n",
      "Iteration 420, loss = 0.12051108\n",
      "Iteration 421, loss = 0.11985324\n",
      "Iteration 422, loss = 0.11919810\n",
      "Iteration 423, loss = 0.11856183\n",
      "Iteration 424, loss = 0.11792058\n",
      "Iteration 425, loss = 0.11727519\n",
      "Iteration 426, loss = 0.11664459\n",
      "Iteration 427, loss = 0.11602637\n",
      "Iteration 428, loss = 0.11540646\n",
      "Iteration 429, loss = 0.11477965\n",
      "Iteration 430, loss = 0.11415876\n",
      "Iteration 431, loss = 0.11355149\n",
      "Iteration 432, loss = 0.11294714\n",
      "Iteration 433, loss = 0.11235436\n",
      "Iteration 434, loss = 0.11176271\n",
      "Iteration 435, loss = 0.11115526\n",
      "Iteration 436, loss = 0.11055903\n",
      "Iteration 437, loss = 0.10998221\n",
      "Iteration 438, loss = 0.10940613\n",
      "Iteration 439, loss = 0.10882219\n",
      "Iteration 440, loss = 0.10824347\n",
      "Iteration 441, loss = 0.10767395\n",
      "Iteration 442, loss = 0.10710999\n",
      "Iteration 443, loss = 0.10654952\n",
      "Iteration 444, loss = 0.10598737\n",
      "Iteration 445, loss = 0.10543444\n",
      "Iteration 446, loss = 0.10488537\n",
      "Iteration 447, loss = 0.10432988\n",
      "Iteration 448, loss = 0.10378170\n",
      "Iteration 449, loss = 0.10324863\n",
      "Iteration 450, loss = 0.10272071\n",
      "Iteration 451, loss = 0.10217840\n",
      "Iteration 452, loss = 0.10164305\n",
      "Iteration 453, loss = 0.10112545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 454, loss = 0.10060345\n",
      "Iteration 455, loss = 0.10008061\n",
      "Iteration 456, loss = 0.09957063\n",
      "Iteration 457, loss = 0.09906968\n",
      "Iteration 458, loss = 0.09855157\n",
      "Iteration 459, loss = 0.09805360\n",
      "Iteration 460, loss = 0.09754054\n",
      "Iteration 461, loss = 0.09703867\n",
      "Iteration 462, loss = 0.09654336\n",
      "Iteration 463, loss = 0.09607220\n",
      "Iteration 464, loss = 0.09557229\n",
      "Iteration 465, loss = 0.09508641\n",
      "Iteration 466, loss = 0.09461132\n",
      "Iteration 467, loss = 0.09412679\n",
      "Iteration 468, loss = 0.09365142\n",
      "Iteration 469, loss = 0.09317903\n",
      "Iteration 470, loss = 0.09270115\n",
      "Iteration 471, loss = 0.09225009\n",
      "Iteration 472, loss = 0.09179357\n",
      "Iteration 473, loss = 0.09132683\n",
      "Iteration 474, loss = 0.09086089\n",
      "Iteration 475, loss = 0.09038828\n",
      "Iteration 476, loss = 0.08991518\n",
      "Iteration 477, loss = 0.08944874\n",
      "Iteration 478, loss = 0.08898533\n",
      "Iteration 479, loss = 0.08851353\n",
      "Iteration 480, loss = 0.08805248\n",
      "Iteration 481, loss = 0.08759368\n",
      "Iteration 482, loss = 0.08713756\n",
      "Iteration 483, loss = 0.08668080\n",
      "Iteration 484, loss = 0.08622072\n",
      "Iteration 485, loss = 0.08577177\n",
      "Iteration 486, loss = 0.08534094\n",
      "Iteration 487, loss = 0.08491109\n",
      "Iteration 488, loss = 0.08449033\n",
      "Iteration 489, loss = 0.08407351\n",
      "Iteration 490, loss = 0.08365538\n",
      "Iteration 491, loss = 0.08322170\n",
      "Iteration 492, loss = 0.08280584\n",
      "Iteration 493, loss = 0.08239715\n",
      "Iteration 494, loss = 0.08197559\n",
      "Iteration 495, loss = 0.08156032\n",
      "Iteration 496, loss = 0.08114563\n",
      "Iteration 497, loss = 0.08073822\n",
      "Iteration 498, loss = 0.08032595\n",
      "Iteration 499, loss = 0.07991948\n",
      "Iteration 500, loss = 0.07952213\n",
      "Iteration 501, loss = 0.07912158\n",
      "Iteration 502, loss = 0.07872572\n",
      "Iteration 503, loss = 0.07832082\n",
      "Iteration 504, loss = 0.07792547\n",
      "Iteration 505, loss = 0.07753783\n",
      "Iteration 506, loss = 0.07716232\n",
      "Iteration 507, loss = 0.07677876\n",
      "Iteration 508, loss = 0.07639328\n",
      "Iteration 509, loss = 0.07601467\n",
      "Iteration 510, loss = 0.07562833\n",
      "Iteration 511, loss = 0.07524528\n",
      "Iteration 512, loss = 0.07487929\n",
      "Iteration 513, loss = 0.07449896\n",
      "Iteration 514, loss = 0.07412898\n",
      "Iteration 515, loss = 0.07376428\n",
      "Iteration 516, loss = 0.07340073\n",
      "Iteration 517, loss = 0.07304250\n",
      "Iteration 518, loss = 0.07268294\n",
      "Iteration 519, loss = 0.07231723\n",
      "Iteration 520, loss = 0.07196410\n",
      "Iteration 521, loss = 0.07160288\n",
      "Iteration 522, loss = 0.07125361\n",
      "Iteration 523, loss = 0.07090169\n",
      "Iteration 524, loss = 0.07054875\n",
      "Iteration 525, loss = 0.07021505\n",
      "Iteration 526, loss = 0.06988260\n",
      "Iteration 527, loss = 0.06954091\n",
      "Iteration 528, loss = 0.06918946\n",
      "Iteration 529, loss = 0.06884917\n",
      "Iteration 530, loss = 0.06852903\n",
      "Iteration 531, loss = 0.06820180\n",
      "Iteration 532, loss = 0.06786466\n",
      "Iteration 533, loss = 0.06753825\n",
      "Iteration 534, loss = 0.06721885\n",
      "Iteration 535, loss = 0.06690104\n",
      "Iteration 536, loss = 0.06657682\n",
      "Iteration 537, loss = 0.06625874\n",
      "Iteration 538, loss = 0.06593894\n",
      "Iteration 539, loss = 0.06562391\n",
      "Iteration 540, loss = 0.06530952\n",
      "Iteration 541, loss = 0.06499802\n",
      "Iteration 542, loss = 0.06468436\n",
      "Iteration 543, loss = 0.06438125\n",
      "Iteration 544, loss = 0.06408005\n",
      "Iteration 545, loss = 0.06377608\n",
      "Iteration 546, loss = 0.06347695\n",
      "Iteration 547, loss = 0.06318241\n",
      "Iteration 548, loss = 0.06288209\n",
      "Iteration 549, loss = 0.06258797\n",
      "Iteration 550, loss = 0.06228932\n",
      "Iteration 551, loss = 0.06200162\n",
      "Iteration 552, loss = 0.06171206\n",
      "Iteration 553, loss = 0.06141856\n",
      "Iteration 554, loss = 0.06113070\n",
      "Iteration 555, loss = 0.06085179\n",
      "Iteration 556, loss = 0.06057220\n",
      "Iteration 557, loss = 0.06029448\n",
      "Iteration 558, loss = 0.06001513\n",
      "Iteration 559, loss = 0.05974026\n",
      "Iteration 560, loss = 0.05945773\n",
      "Iteration 561, loss = 0.05918551\n",
      "Iteration 562, loss = 0.05891403\n",
      "Iteration 563, loss = 0.05864644\n",
      "Iteration 564, loss = 0.05837676\n",
      "Iteration 565, loss = 0.05811263\n",
      "Iteration 566, loss = 0.05784692\n",
      "Iteration 567, loss = 0.05758171\n",
      "Iteration 568, loss = 0.05732189\n",
      "Iteration 569, loss = 0.05707037\n",
      "Iteration 570, loss = 0.05680942\n",
      "Iteration 571, loss = 0.05654525\n",
      "Iteration 572, loss = 0.05629100\n",
      "Iteration 573, loss = 0.05604510\n",
      "Iteration 574, loss = 0.05579060\n",
      "Iteration 575, loss = 0.05553427\n",
      "Iteration 576, loss = 0.05527935\n",
      "Iteration 577, loss = 0.05503539\n",
      "Iteration 578, loss = 0.05479038\n",
      "Iteration 579, loss = 0.05454314\n",
      "Iteration 580, loss = 0.05429366\n",
      "Iteration 581, loss = 0.05405499\n",
      "Iteration 582, loss = 0.05382492\n",
      "Iteration 583, loss = 0.05357919\n",
      "Iteration 584, loss = 0.05333392\n",
      "Iteration 585, loss = 0.05311043\n",
      "Iteration 586, loss = 0.05287973\n",
      "Iteration 587, loss = 0.05264285\n",
      "Iteration 588, loss = 0.05240863\n",
      "Iteration 589, loss = 0.05217914\n",
      "Iteration 590, loss = 0.05195158\n",
      "Iteration 591, loss = 0.05172399\n",
      "Iteration 592, loss = 0.05149608\n",
      "Iteration 593, loss = 0.05126826\n",
      "Iteration 594, loss = 0.05104976\n",
      "Iteration 595, loss = 0.05082546\n",
      "Iteration 596, loss = 0.05060847\n",
      "Iteration 597, loss = 0.05039192\n",
      "Iteration 598, loss = 0.05017546\n",
      "Iteration 599, loss = 0.04995306\n",
      "Iteration 600, loss = 0.04974611\n",
      "Iteration 601, loss = 0.04953102\n",
      "Iteration 602, loss = 0.04931323\n",
      "Iteration 603, loss = 0.04910141\n",
      "Iteration 604, loss = 0.04889759\n",
      "Iteration 605, loss = 0.04868972\n",
      "Iteration 606, loss = 0.04847898\n",
      "Iteration 607, loss = 0.04826910\n",
      "Iteration 608, loss = 0.04806825\n",
      "Iteration 609, loss = 0.04786571\n",
      "Iteration 610, loss = 0.04766157\n",
      "Iteration 611, loss = 0.04745457\n",
      "Iteration 612, loss = 0.04725437\n",
      "Iteration 613, loss = 0.04705922\n",
      "Iteration 614, loss = 0.04686205\n",
      "Iteration 615, loss = 0.04666681\n",
      "Iteration 616, loss = 0.04647016\n",
      "Iteration 617, loss = 0.04627137\n",
      "Iteration 618, loss = 0.04608357\n",
      "Iteration 619, loss = 0.04589262\n",
      "Iteration 620, loss = 0.04569717\n",
      "Iteration 621, loss = 0.04550878\n",
      "Iteration 622, loss = 0.04532282\n",
      "Iteration 623, loss = 0.04513558\n",
      "Iteration 624, loss = 0.04494560\n",
      "Iteration 625, loss = 0.04476222\n",
      "Iteration 626, loss = 0.04458184\n",
      "Iteration 627, loss = 0.04439601\n",
      "Iteration 628, loss = 0.04421170\n",
      "Iteration 629, loss = 0.04402674\n",
      "Iteration 630, loss = 0.04384753\n",
      "Iteration 631, loss = 0.04367247\n",
      "Iteration 632, loss = 0.04349387\n",
      "Iteration 633, loss = 0.04330782\n",
      "Iteration 634, loss = 0.04313453\n",
      "Iteration 635, loss = 0.04295900\n",
      "Iteration 636, loss = 0.04277943\n",
      "Iteration 637, loss = 0.04259731\n",
      "Iteration 638, loss = 0.04240906\n",
      "Iteration 639, loss = 0.04223090\n",
      "Iteration 640, loss = 0.04205139\n",
      "Iteration 641, loss = 0.04187347\n",
      "Iteration 642, loss = 0.04170553\n",
      "Iteration 643, loss = 0.04153363\n",
      "Iteration 644, loss = 0.04135714\n",
      "Iteration 645, loss = 0.04119193\n",
      "Iteration 646, loss = 0.04102410\n",
      "Iteration 647, loss = 0.04085149\n",
      "Iteration 648, loss = 0.04068078\n",
      "Iteration 649, loss = 0.04051559\n",
      "Iteration 650, loss = 0.04034818\n",
      "Iteration 651, loss = 0.04017754\n",
      "Iteration 652, loss = 0.04001221\n",
      "Iteration 653, loss = 0.03984737\n",
      "Iteration 654, loss = 0.03968018\n",
      "Iteration 655, loss = 0.03951620\n",
      "Iteration 656, loss = 0.03935735\n",
      "Iteration 657, loss = 0.03919017\n",
      "Iteration 658, loss = 0.03902649\n",
      "Iteration 659, loss = 0.03887233\n",
      "Iteration 660, loss = 0.03871397\n",
      "Iteration 661, loss = 0.03855707\n",
      "Iteration 662, loss = 0.03839784\n",
      "Iteration 663, loss = 0.03823420\n",
      "Iteration 664, loss = 0.03807793\n",
      "Iteration 665, loss = 0.03792354\n",
      "Iteration 666, loss = 0.03776549\n",
      "Iteration 667, loss = 0.03761424\n",
      "Iteration 668, loss = 0.03746263\n",
      "Iteration 669, loss = 0.03730841\n",
      "Iteration 670, loss = 0.03715144\n",
      "Iteration 671, loss = 0.03700199\n",
      "Iteration 672, loss = 0.03685148\n",
      "Iteration 673, loss = 0.03669753\n",
      "Iteration 674, loss = 0.03654593\n",
      "Iteration 675, loss = 0.03639629\n",
      "Iteration 676, loss = 0.03624586\n",
      "Iteration 677, loss = 0.03610249\n",
      "Iteration 678, loss = 0.03596344\n",
      "Iteration 679, loss = 0.03581997\n",
      "Iteration 680, loss = 0.03567404\n",
      "Iteration 681, loss = 0.03553104\n",
      "Iteration 682, loss = 0.03539283\n",
      "Iteration 683, loss = 0.03525412\n",
      "Iteration 684, loss = 0.03510985\n",
      "Iteration 685, loss = 0.03496652\n",
      "Iteration 686, loss = 0.03482624\n",
      "Iteration 687, loss = 0.03468617\n",
      "Iteration 688, loss = 0.03454426\n",
      "Iteration 689, loss = 0.03440903\n",
      "Iteration 690, loss = 0.03427266\n",
      "Iteration 691, loss = 0.03413544\n",
      "Iteration 692, loss = 0.03400150\n",
      "Iteration 693, loss = 0.03386675\n",
      "Iteration 694, loss = 0.03373064\n",
      "Iteration 695, loss = 0.03359465\n",
      "Iteration 696, loss = 0.03346393\n",
      "Iteration 697, loss = 0.03332999\n",
      "Iteration 698, loss = 0.03319595\n",
      "Iteration 699, loss = 0.03306630\n",
      "Iteration 700, loss = 0.03293655\n",
      "Iteration 701, loss = 0.03280941\n",
      "Iteration 702, loss = 0.03268223\n",
      "Iteration 703, loss = 0.03255220\n",
      "Iteration 704, loss = 0.03242496\n",
      "Iteration 705, loss = 0.03229883\n",
      "Iteration 706, loss = 0.03217136\n",
      "Iteration 707, loss = 0.03204566\n",
      "Iteration 708, loss = 0.03191996\n",
      "Iteration 709, loss = 0.03179618\n",
      "Iteration 710, loss = 0.03167378\n",
      "Iteration 711, loss = 0.03154862\n",
      "Iteration 712, loss = 0.03142523\n",
      "Iteration 713, loss = 0.03130357\n",
      "Iteration 714, loss = 0.03118355\n",
      "Iteration 715, loss = 0.03106526\n",
      "Iteration 716, loss = 0.03094341\n",
      "Iteration 717, loss = 0.03082354\n",
      "Iteration 718, loss = 0.03070668\n",
      "Iteration 719, loss = 0.03058697\n",
      "Iteration 720, loss = 0.03047243\n",
      "Iteration 721, loss = 0.03035475\n",
      "Iteration 722, loss = 0.03023576\n",
      "Iteration 723, loss = 0.03012381\n",
      "Iteration 724, loss = 0.03001058\n",
      "Iteration 725, loss = 0.02989320\n",
      "Iteration 726, loss = 0.02977799\n",
      "Iteration 727, loss = 0.02966509\n",
      "Iteration 728, loss = 0.02955581\n",
      "Iteration 729, loss = 0.02944318\n",
      "Iteration 730, loss = 0.02932627\n",
      "Iteration 731, loss = 0.02922145\n",
      "Iteration 732, loss = 0.02911350\n",
      "Iteration 733, loss = 0.02900082\n",
      "Iteration 734, loss = 0.02888931\n",
      "Iteration 735, loss = 0.02878098\n",
      "Iteration 736, loss = 0.02867369\n",
      "Iteration 737, loss = 0.02856790\n",
      "Iteration 738, loss = 0.02846200\n",
      "Iteration 739, loss = 0.02835428\n",
      "Iteration 740, loss = 0.02824771\n",
      "Iteration 741, loss = 0.02814321\n",
      "Iteration 742, loss = 0.02803747\n",
      "Iteration 743, loss = 0.02793685\n",
      "Iteration 744, loss = 0.02783429\n",
      "Iteration 745, loss = 0.02772871\n",
      "Iteration 746, loss = 0.02762319\n",
      "Iteration 747, loss = 0.02752445\n",
      "Iteration 748, loss = 0.02742555\n",
      "Iteration 749, loss = 0.02732468\n",
      "Iteration 750, loss = 0.02722178\n",
      "Iteration 751, loss = 0.02711973\n",
      "Iteration 752, loss = 0.02702329\n",
      "Iteration 753, loss = 0.02692392\n",
      "Iteration 754, loss = 0.02682661\n",
      "Iteration 755, loss = 0.02672599\n",
      "Iteration 756, loss = 0.02663002\n",
      "Iteration 757, loss = 0.02653602\n",
      "Iteration 758, loss = 0.02643956\n",
      "Iteration 759, loss = 0.02633941\n",
      "Iteration 760, loss = 0.02624244\n",
      "Iteration 761, loss = 0.02615112\n",
      "Iteration 762, loss = 0.02605735\n",
      "Iteration 763, loss = 0.02596047\n",
      "Iteration 764, loss = 0.02586580\n",
      "Iteration 765, loss = 0.02577345\n",
      "Iteration 766, loss = 0.02568346\n",
      "Iteration 767, loss = 0.02559027\n",
      "Iteration 768, loss = 0.02549821\n",
      "Iteration 769, loss = 0.02541025\n",
      "Iteration 770, loss = 0.02532050\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(max_iter=1000, verbose=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp.fit(x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10197525",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T06:40:22.168771Z",
     "start_time": "2021-12-27T06:40:22.155769Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp.predict(x_data) # x_data = (4,2) w = (2, 100)  x_data @ w = (4,100) x_DATA @ 100 * 1 = (4*1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fd018f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T06:40:22.183775Z",
     "start_time": "2021-12-27T06:40:22.169771Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_mlp.coefs_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d90d30f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T06:40:22.198779Z",
     "start_time": "2021-12-27T06:40:22.184778Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 100)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp.coefs_[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc9010f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T06:40:22.213789Z",
     "start_time": "2021-12-27T06:40:22.199780Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp.score(x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0472c3d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T06:40:22.393830Z",
     "start_time": "2021-12-27T06:40:22.214783Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.70272275\n",
      "Iteration 2, loss = 0.69576948\n",
      "Iteration 3, loss = 0.68941623\n",
      "Iteration 4, loss = 0.68347509\n",
      "Iteration 5, loss = 0.67795941\n",
      "Iteration 6, loss = 0.67310916\n",
      "Iteration 7, loss = 0.66851991\n",
      "Iteration 8, loss = 0.66415672\n",
      "Iteration 9, loss = 0.66035275\n",
      "Iteration 10, loss = 0.65692036\n",
      "Iteration 11, loss = 0.65353779\n",
      "Iteration 12, loss = 0.65017825\n",
      "Iteration 13, loss = 0.64670388\n",
      "Iteration 14, loss = 0.64327055\n",
      "Iteration 15, loss = 0.64010072\n",
      "Iteration 16, loss = 0.63683215\n",
      "Iteration 17, loss = 0.63331672\n",
      "Iteration 18, loss = 0.62967376\n",
      "Iteration 19, loss = 0.62591468\n",
      "Iteration 20, loss = 0.62206154\n",
      "Iteration 21, loss = 0.61830692\n",
      "Iteration 22, loss = 0.61464020\n",
      "Iteration 23, loss = 0.61076468\n",
      "Iteration 24, loss = 0.60682700\n",
      "Iteration 25, loss = 0.60282444\n",
      "Iteration 26, loss = 0.59875892\n",
      "Iteration 27, loss = 0.59470608\n",
      "Iteration 28, loss = 0.59061033\n",
      "Iteration 29, loss = 0.58646705\n",
      "Iteration 30, loss = 0.58242342\n",
      "Iteration 31, loss = 0.57828797\n",
      "Iteration 32, loss = 0.57399772\n",
      "Iteration 33, loss = 0.56960139\n",
      "Iteration 34, loss = 0.56505245\n",
      "Iteration 35, loss = 0.56049411\n",
      "Iteration 36, loss = 0.55602471\n",
      "Iteration 37, loss = 0.55137099\n",
      "Iteration 38, loss = 0.54661761\n",
      "Iteration 39, loss = 0.54180142\n",
      "Iteration 40, loss = 0.53682314\n",
      "Iteration 41, loss = 0.53176678\n",
      "Iteration 42, loss = 0.52666491\n",
      "Iteration 43, loss = 0.52135534\n",
      "Iteration 44, loss = 0.51601634\n",
      "Iteration 45, loss = 0.51061162\n",
      "Iteration 46, loss = 0.50515400\n",
      "Iteration 47, loss = 0.49961875\n",
      "Iteration 48, loss = 0.49397440\n",
      "Iteration 49, loss = 0.48806435\n",
      "Iteration 50, loss = 0.48208033\n",
      "Iteration 51, loss = 0.47605075\n",
      "Iteration 52, loss = 0.46975405\n",
      "Iteration 53, loss = 0.46330669\n",
      "Iteration 54, loss = 0.45689711\n",
      "Iteration 55, loss = 0.45037814\n",
      "Iteration 56, loss = 0.44384096\n",
      "Iteration 57, loss = 0.43748906\n",
      "Iteration 58, loss = 0.43100167\n",
      "Iteration 59, loss = 0.42439904\n",
      "Iteration 60, loss = 0.41758337\n",
      "Iteration 61, loss = 0.41077808\n",
      "Iteration 62, loss = 0.40388049\n",
      "Iteration 63, loss = 0.39700569\n",
      "Iteration 64, loss = 0.39007868\n",
      "Iteration 65, loss = 0.38310235\n",
      "Iteration 66, loss = 0.37620845\n",
      "Iteration 67, loss = 0.36936734\n",
      "Iteration 68, loss = 0.36238452\n",
      "Iteration 69, loss = 0.35553285\n",
      "Iteration 70, loss = 0.34868150\n",
      "Iteration 71, loss = 0.34181908\n",
      "Iteration 72, loss = 0.33493612\n",
      "Iteration 73, loss = 0.32796634\n",
      "Iteration 74, loss = 0.32110753\n",
      "Iteration 75, loss = 0.31437874\n",
      "Iteration 76, loss = 0.30756248\n",
      "Iteration 77, loss = 0.30073311\n",
      "Iteration 78, loss = 0.29397157\n",
      "Iteration 79, loss = 0.28727867\n",
      "Iteration 80, loss = 0.28068751\n",
      "Iteration 81, loss = 0.27411734\n",
      "Iteration 82, loss = 0.26759317\n",
      "Iteration 83, loss = 0.26116667\n",
      "Iteration 84, loss = 0.25488219\n",
      "Iteration 85, loss = 0.24860277\n",
      "Iteration 86, loss = 0.24246088\n",
      "Iteration 87, loss = 0.23636107\n",
      "Iteration 88, loss = 0.23032986\n",
      "Iteration 89, loss = 0.22433191\n",
      "Iteration 90, loss = 0.21847956\n",
      "Iteration 91, loss = 0.21277437\n",
      "Iteration 92, loss = 0.20721575\n",
      "Iteration 93, loss = 0.20174757\n",
      "Iteration 94, loss = 0.19625957\n",
      "Iteration 95, loss = 0.19093243\n",
      "Iteration 96, loss = 0.18569902\n",
      "Iteration 97, loss = 0.18060860\n",
      "Iteration 98, loss = 0.17561536\n",
      "Iteration 99, loss = 0.17079410\n",
      "Iteration 100, loss = 0.16599815\n",
      "Iteration 101, loss = 0.16134639\n",
      "Iteration 102, loss = 0.15684756\n",
      "Iteration 103, loss = 0.15248630\n",
      "Iteration 104, loss = 0.14817874\n",
      "Iteration 105, loss = 0.14397094\n",
      "Iteration 106, loss = 0.13987329\n",
      "Iteration 107, loss = 0.13594011\n",
      "Iteration 108, loss = 0.13204467\n",
      "Iteration 109, loss = 0.12825922\n",
      "Iteration 110, loss = 0.12461630\n",
      "Iteration 111, loss = 0.12116606\n",
      "Iteration 112, loss = 0.11770427\n",
      "Iteration 113, loss = 0.11433290\n",
      "Iteration 114, loss = 0.11110731\n",
      "Iteration 115, loss = 0.10799688\n",
      "Iteration 116, loss = 0.10495924\n",
      "Iteration 117, loss = 0.10202159\n",
      "Iteration 118, loss = 0.09917707\n",
      "Iteration 119, loss = 0.09641642\n",
      "Iteration 120, loss = 0.09375584\n",
      "Iteration 121, loss = 0.09115022\n",
      "Iteration 122, loss = 0.08863882\n",
      "Iteration 123, loss = 0.08619712\n",
      "Iteration 124, loss = 0.08384247\n",
      "Iteration 125, loss = 0.08154858\n",
      "Iteration 126, loss = 0.07936802\n",
      "Iteration 127, loss = 0.07722406\n",
      "Iteration 128, loss = 0.07517496\n",
      "Iteration 129, loss = 0.07318533\n",
      "Iteration 130, loss = 0.07126305\n",
      "Iteration 131, loss = 0.06939742\n",
      "Iteration 132, loss = 0.06762415\n",
      "Iteration 133, loss = 0.06589710\n",
      "Iteration 134, loss = 0.06419949\n",
      "Iteration 135, loss = 0.06258984\n",
      "Iteration 136, loss = 0.06102022\n",
      "Iteration 137, loss = 0.05948469\n",
      "Iteration 138, loss = 0.05801490\n",
      "Iteration 139, loss = 0.05658399\n",
      "Iteration 140, loss = 0.05519808\n",
      "Iteration 141, loss = 0.05388392\n",
      "Iteration 142, loss = 0.05259969\n",
      "Iteration 143, loss = 0.05135816\n",
      "Iteration 144, loss = 0.05016517\n",
      "Iteration 145, loss = 0.04900559\n",
      "Iteration 146, loss = 0.04787762\n",
      "Iteration 147, loss = 0.04679588\n",
      "Iteration 148, loss = 0.04573837\n",
      "Iteration 149, loss = 0.04471672\n",
      "Iteration 150, loss = 0.04373266\n",
      "Iteration 151, loss = 0.04277358\n",
      "Iteration 152, loss = 0.04184382\n",
      "Iteration 153, loss = 0.04094347\n",
      "Iteration 154, loss = 0.04007386\n",
      "Iteration 155, loss = 0.03924232\n",
      "Iteration 156, loss = 0.03842377\n",
      "Iteration 157, loss = 0.03762255\n",
      "Iteration 158, loss = 0.03685788\n",
      "Iteration 159, loss = 0.03611519\n",
      "Iteration 160, loss = 0.03539558\n",
      "Iteration 161, loss = 0.03469866\n",
      "Iteration 162, loss = 0.03401887\n",
      "Iteration 163, loss = 0.03335703\n",
      "Iteration 164, loss = 0.03271445\n",
      "Iteration 165, loss = 0.03209286\n",
      "Iteration 166, loss = 0.03148576\n",
      "Iteration 167, loss = 0.03090295\n",
      "Iteration 168, loss = 0.03034404\n",
      "Iteration 169, loss = 0.02979187\n",
      "Iteration 170, loss = 0.02924920\n",
      "Iteration 171, loss = 0.02872694\n",
      "Iteration 172, loss = 0.02822093\n",
      "Iteration 173, loss = 0.02772340\n",
      "Iteration 174, loss = 0.02724388\n",
      "Iteration 175, loss = 0.02677498\n",
      "Iteration 176, loss = 0.02631562\n",
      "Iteration 177, loss = 0.02586945\n",
      "Iteration 178, loss = 0.02544196\n",
      "Iteration 179, loss = 0.02502350\n",
      "Iteration 180, loss = 0.02461269\n",
      "Iteration 181, loss = 0.02421305\n",
      "Iteration 182, loss = 0.02382418\n",
      "Iteration 183, loss = 0.02344685\n",
      "Iteration 184, loss = 0.02307555\n",
      "Iteration 185, loss = 0.02271788\n",
      "Iteration 186, loss = 0.02236446\n",
      "Iteration 187, loss = 0.02201960\n",
      "Iteration 188, loss = 0.02168603\n",
      "Iteration 189, loss = 0.02136079\n",
      "Iteration 190, loss = 0.02104551\n",
      "Iteration 191, loss = 0.02073778\n",
      "Iteration 192, loss = 0.02042972\n",
      "Iteration 193, loss = 0.02013153\n",
      "Iteration 194, loss = 0.01984594\n",
      "Iteration 195, loss = 0.01956704\n",
      "Iteration 196, loss = 0.01929204\n",
      "Iteration 197, loss = 0.01902356\n",
      "Iteration 198, loss = 0.01875913\n",
      "Iteration 199, loss = 0.01850135\n",
      "Iteration 200, loss = 0.01824837\n",
      "Iteration 201, loss = 0.01800097\n",
      "Iteration 202, loss = 0.01776174\n",
      "Iteration 203, loss = 0.01752546\n",
      "Iteration 204, loss = 0.01729570\n",
      "Iteration 205, loss = 0.01707077\n",
      "Iteration 206, loss = 0.01685049\n",
      "Iteration 207, loss = 0.01663488\n",
      "Iteration 208, loss = 0.01642348\n",
      "Iteration 209, loss = 0.01621518\n",
      "Iteration 210, loss = 0.01601235\n",
      "Iteration 211, loss = 0.01581434\n",
      "Iteration 212, loss = 0.01562069\n",
      "Iteration 213, loss = 0.01543044\n",
      "Iteration 214, loss = 0.01524344\n",
      "Iteration 215, loss = 0.01506177\n",
      "Iteration 216, loss = 0.01488185\n",
      "Iteration 217, loss = 0.01470568\n",
      "Iteration 218, loss = 0.01453369\n",
      "Iteration 219, loss = 0.01436625\n",
      "Iteration 220, loss = 0.01420064\n",
      "Iteration 221, loss = 0.01403907\n",
      "Iteration 222, loss = 0.01388016\n",
      "Iteration 223, loss = 0.01372282\n",
      "Iteration 224, loss = 0.01356928\n",
      "Iteration 225, loss = 0.01341954\n",
      "Iteration 226, loss = 0.01327263\n",
      "Iteration 227, loss = 0.01312975\n",
      "Iteration 228, loss = 0.01298721\n",
      "Iteration 229, loss = 0.01284949\n",
      "Iteration 230, loss = 0.01271223\n",
      "Iteration 231, loss = 0.01257754\n",
      "Iteration 232, loss = 0.01244510\n",
      "Iteration 233, loss = 0.01231496\n",
      "Iteration 234, loss = 0.01218745\n",
      "Iteration 235, loss = 0.01206209\n",
      "Iteration 236, loss = 0.01193925\n",
      "Iteration 237, loss = 0.01181883\n",
      "Iteration 238, loss = 0.01170045\n",
      "Iteration 239, loss = 0.01158467\n",
      "Iteration 240, loss = 0.01146869\n",
      "Iteration 241, loss = 0.01135494\n",
      "Iteration 242, loss = 0.01124516\n",
      "Iteration 243, loss = 0.01113734\n",
      "Iteration 244, loss = 0.01102989\n",
      "Iteration 245, loss = 0.01092439\n",
      "Iteration 246, loss = 0.01082189\n",
      "Iteration 247, loss = 0.01072092\n",
      "Iteration 248, loss = 0.01061843\n",
      "Iteration 249, loss = 0.01052046\n",
      "Iteration 250, loss = 0.01042431\n",
      "Iteration 251, loss = 0.01032919\n",
      "Iteration 252, loss = 0.01023491\n",
      "Iteration 253, loss = 0.01014175\n",
      "Iteration 254, loss = 0.01005124\n",
      "Iteration 255, loss = 0.00996305\n",
      "Iteration 256, loss = 0.00987491\n",
      "Iteration 257, loss = 0.00978778\n",
      "Iteration 258, loss = 0.00970138\n",
      "Iteration 259, loss = 0.00961709\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\13a71\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000, verbose=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp1 = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000, verbose = True) # 4 *2 2*100\n",
    "model_mlp1.fit(x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f453aed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T06:40:22.408846Z",
     "start_time": "2021-12-27T06:40:22.394837Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 100)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##feature data의 수, weight의 수\n",
    "model_mlp1.coefs_[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d071d139",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T06:40:22.423855Z",
     "start_time": "2021-12-27T06:40:22.410827Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 50)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## weight의 수 * wight2의 수\n",
    "model_mlp1.coefs_[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b771c3cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T06:40:22.438851Z",
     "start_time": "2021-12-27T06:40:22.424848Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wight2의 수 * label\n",
    "model_mlp1.coefs_[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4554584",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T06:40:22.453854Z",
     "start_time": "2021-12-27T06:40:22.439851Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp1.predict(x_data) # 4*2 @  2* 100 = 4* 100 / 100 * 50 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e583329c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T06:40:22.468857Z",
     "start_time": "2021-12-27T06:40:22.454854Z"
    }
   },
   "outputs": [],
   "source": [
    "model_mlp2 = MLPClassifier(hidden_layer_sizes= (64, 32, 16),\n",
    "                          max_iter= 1000, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed80a210",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T06:40:22.693907Z",
     "start_time": "2021-12-27T06:40:22.470859Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\13a71\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.70057649\n",
      "Iteration 2, loss = 0.69699104\n",
      "Iteration 3, loss = 0.69358636\n",
      "Iteration 4, loss = 0.69043317\n",
      "Iteration 5, loss = 0.68747478\n",
      "Iteration 6, loss = 0.68461984\n",
      "Iteration 7, loss = 0.68202561\n",
      "Iteration 8, loss = 0.67967773\n",
      "Iteration 9, loss = 0.67725884\n",
      "Iteration 10, loss = 0.67448531\n",
      "Iteration 11, loss = 0.67160152\n",
      "Iteration 12, loss = 0.66910226\n",
      "Iteration 13, loss = 0.66681960\n",
      "Iteration 14, loss = 0.66451196\n",
      "Iteration 15, loss = 0.66235519\n",
      "Iteration 16, loss = 0.66019914\n",
      "Iteration 17, loss = 0.65794235\n",
      "Iteration 18, loss = 0.65571934\n",
      "Iteration 19, loss = 0.65356089\n",
      "Iteration 20, loss = 0.65161808\n",
      "Iteration 21, loss = 0.64928424\n",
      "Iteration 22, loss = 0.64668587\n",
      "Iteration 23, loss = 0.64418337\n",
      "Iteration 24, loss = 0.64195884\n",
      "Iteration 25, loss = 0.64015280\n",
      "Iteration 26, loss = 0.63799691\n",
      "Iteration 27, loss = 0.63581171\n",
      "Iteration 28, loss = 0.63355067\n",
      "Iteration 29, loss = 0.63099540\n",
      "Iteration 30, loss = 0.62852114\n",
      "Iteration 31, loss = 0.62648326\n",
      "Iteration 32, loss = 0.62417238\n",
      "Iteration 33, loss = 0.62181365\n",
      "Iteration 34, loss = 0.61952208\n",
      "Iteration 35, loss = 0.61710918\n",
      "Iteration 36, loss = 0.61462765\n",
      "Iteration 37, loss = 0.61197486\n",
      "Iteration 38, loss = 0.60946387\n",
      "Iteration 39, loss = 0.60681077\n",
      "Iteration 40, loss = 0.60410579\n",
      "Iteration 41, loss = 0.60134020\n",
      "Iteration 42, loss = 0.59873190\n",
      "Iteration 43, loss = 0.59599046\n",
      "Iteration 44, loss = 0.59317136\n",
      "Iteration 45, loss = 0.59035122\n",
      "Iteration 46, loss = 0.58742122\n",
      "Iteration 47, loss = 0.58461505\n",
      "Iteration 48, loss = 0.58158398\n",
      "Iteration 49, loss = 0.57848096\n",
      "Iteration 50, loss = 0.57526501\n",
      "Iteration 51, loss = 0.57223519\n",
      "Iteration 52, loss = 0.56904507\n",
      "Iteration 53, loss = 0.56569907\n",
      "Iteration 54, loss = 0.56228211\n",
      "Iteration 55, loss = 0.55879049\n",
      "Iteration 56, loss = 0.55534794\n",
      "Iteration 57, loss = 0.55196109\n",
      "Iteration 58, loss = 0.54847775\n",
      "Iteration 59, loss = 0.54471115\n",
      "Iteration 60, loss = 0.54084453\n",
      "Iteration 61, loss = 0.53694787\n",
      "Iteration 62, loss = 0.53305817\n",
      "Iteration 63, loss = 0.52906502\n",
      "Iteration 64, loss = 0.52507855\n",
      "Iteration 65, loss = 0.52116751\n",
      "Iteration 66, loss = 0.51702358\n",
      "Iteration 67, loss = 0.51259329\n",
      "Iteration 68, loss = 0.50815916\n",
      "Iteration 69, loss = 0.50365039\n",
      "Iteration 70, loss = 0.49916372\n",
      "Iteration 71, loss = 0.49473550\n",
      "Iteration 72, loss = 0.49008028\n",
      "Iteration 73, loss = 0.48539704\n",
      "Iteration 74, loss = 0.48054146\n",
      "Iteration 75, loss = 0.47569636\n",
      "Iteration 76, loss = 0.47070952\n",
      "Iteration 77, loss = 0.46573989\n",
      "Iteration 78, loss = 0.46054908\n",
      "Iteration 79, loss = 0.45539709\n",
      "Iteration 80, loss = 0.45024341\n",
      "Iteration 81, loss = 0.44492575\n",
      "Iteration 82, loss = 0.43950142\n",
      "Iteration 83, loss = 0.43408906\n",
      "Iteration 84, loss = 0.42861937\n",
      "Iteration 85, loss = 0.42315657\n",
      "Iteration 86, loss = 0.41769327\n",
      "Iteration 87, loss = 0.41190639\n",
      "Iteration 88, loss = 0.40622628\n",
      "Iteration 89, loss = 0.40057388\n",
      "Iteration 90, loss = 0.39466488\n",
      "Iteration 91, loss = 0.38900049\n",
      "Iteration 92, loss = 0.38310446\n",
      "Iteration 93, loss = 0.37714657\n",
      "Iteration 94, loss = 0.37120149\n",
      "Iteration 95, loss = 0.36515638\n",
      "Iteration 96, loss = 0.35904205\n",
      "Iteration 97, loss = 0.35290688\n",
      "Iteration 98, loss = 0.34698215\n",
      "Iteration 99, loss = 0.34084631\n",
      "Iteration 100, loss = 0.33467345\n",
      "Iteration 101, loss = 0.32852716\n",
      "Iteration 102, loss = 0.32284969\n",
      "Iteration 103, loss = 0.31689687\n",
      "Iteration 104, loss = 0.31096733\n",
      "Iteration 105, loss = 0.30493080\n",
      "Iteration 106, loss = 0.29916848\n",
      "Iteration 107, loss = 0.29334513\n",
      "Iteration 108, loss = 0.28749330\n",
      "Iteration 109, loss = 0.28177277\n",
      "Iteration 110, loss = 0.27594074\n",
      "Iteration 111, loss = 0.27008130\n",
      "Iteration 112, loss = 0.26437330\n",
      "Iteration 113, loss = 0.25868495\n",
      "Iteration 114, loss = 0.25300944\n",
      "Iteration 115, loss = 0.24743757\n",
      "Iteration 116, loss = 0.24185588\n",
      "Iteration 117, loss = 0.23641162\n",
      "Iteration 118, loss = 0.23109781\n",
      "Iteration 119, loss = 0.22573549\n",
      "Iteration 120, loss = 0.22028030\n",
      "Iteration 121, loss = 0.21511700\n",
      "Iteration 122, loss = 0.21004431\n",
      "Iteration 123, loss = 0.20508315\n",
      "Iteration 124, loss = 0.20014516\n",
      "Iteration 125, loss = 0.19525874\n",
      "Iteration 126, loss = 0.19046006\n",
      "Iteration 127, loss = 0.18566142\n",
      "Iteration 128, loss = 0.18103438\n",
      "Iteration 129, loss = 0.17642292\n",
      "Iteration 130, loss = 0.17187033\n",
      "Iteration 131, loss = 0.16733303\n",
      "Iteration 132, loss = 0.16302976\n",
      "Iteration 133, loss = 0.15871114\n",
      "Iteration 134, loss = 0.15455383\n",
      "Iteration 135, loss = 0.15044028\n",
      "Iteration 136, loss = 0.14642286\n",
      "Iteration 137, loss = 0.14250034\n",
      "Iteration 138, loss = 0.13865294\n",
      "Iteration 139, loss = 0.13500549\n",
      "Iteration 140, loss = 0.13143679\n",
      "Iteration 141, loss = 0.12786213\n",
      "Iteration 142, loss = 0.12440611\n",
      "Iteration 143, loss = 0.12105652\n",
      "Iteration 144, loss = 0.11772168\n",
      "Iteration 145, loss = 0.11452815\n",
      "Iteration 146, loss = 0.11140907\n",
      "Iteration 147, loss = 0.10838877\n",
      "Iteration 148, loss = 0.10536334\n",
      "Iteration 149, loss = 0.10239717\n",
      "Iteration 150, loss = 0.09955882\n",
      "Iteration 151, loss = 0.09678607\n",
      "Iteration 152, loss = 0.09411181\n",
      "Iteration 153, loss = 0.09152991\n",
      "Iteration 154, loss = 0.08898040\n",
      "Iteration 155, loss = 0.08650217\n",
      "Iteration 156, loss = 0.08410417\n",
      "Iteration 157, loss = 0.08177725\n",
      "Iteration 158, loss = 0.07949896\n",
      "Iteration 159, loss = 0.07729037\n",
      "Iteration 160, loss = 0.07517812\n",
      "Iteration 161, loss = 0.07310941\n",
      "Iteration 162, loss = 0.07109495\n",
      "Iteration 163, loss = 0.06916056\n",
      "Iteration 164, loss = 0.06728485\n",
      "Iteration 165, loss = 0.06542998\n",
      "Iteration 166, loss = 0.06366688\n",
      "Iteration 167, loss = 0.06194905\n",
      "Iteration 168, loss = 0.06030744\n",
      "Iteration 169, loss = 0.05870515\n",
      "Iteration 170, loss = 0.05715624\n",
      "Iteration 171, loss = 0.05566553\n",
      "Iteration 172, loss = 0.05419494\n",
      "Iteration 173, loss = 0.05280139\n",
      "Iteration 174, loss = 0.05143686\n",
      "Iteration 175, loss = 0.05012967\n",
      "Iteration 176, loss = 0.04889632\n",
      "Iteration 177, loss = 0.04768046\n",
      "Iteration 178, loss = 0.04648569\n",
      "Iteration 179, loss = 0.04534229\n",
      "Iteration 180, loss = 0.04425741\n",
      "Iteration 181, loss = 0.04319534\n",
      "Iteration 182, loss = 0.04216014\n",
      "Iteration 183, loss = 0.04115502\n",
      "Iteration 184, loss = 0.04018563\n",
      "Iteration 185, loss = 0.03924112\n",
      "Iteration 186, loss = 0.03833630\n",
      "Iteration 187, loss = 0.03745217\n",
      "Iteration 188, loss = 0.03659172\n",
      "Iteration 189, loss = 0.03576397\n",
      "Iteration 190, loss = 0.03497708\n",
      "Iteration 191, loss = 0.03420222\n",
      "Iteration 192, loss = 0.03345236\n",
      "Iteration 193, loss = 0.03273851\n",
      "Iteration 194, loss = 0.03205397\n",
      "Iteration 195, loss = 0.03137653\n",
      "Iteration 196, loss = 0.03071302\n",
      "Iteration 197, loss = 0.03007016\n",
      "Iteration 198, loss = 0.02944525\n",
      "Iteration 199, loss = 0.02884876\n",
      "Iteration 200, loss = 0.02827668\n",
      "Iteration 201, loss = 0.02771761\n",
      "Iteration 202, loss = 0.02717557\n",
      "Iteration 203, loss = 0.02664487\n",
      "Iteration 204, loss = 0.02613646\n",
      "Iteration 205, loss = 0.02564352\n",
      "Iteration 206, loss = 0.02516061\n",
      "Iteration 207, loss = 0.02469224\n",
      "Iteration 208, loss = 0.02423881\n",
      "Iteration 209, loss = 0.02380490\n",
      "Iteration 210, loss = 0.02337600\n",
      "Iteration 211, loss = 0.02295728\n",
      "Iteration 212, loss = 0.02255144\n",
      "Iteration 213, loss = 0.02215557\n",
      "Iteration 214, loss = 0.02177205\n",
      "Iteration 215, loss = 0.02139511\n",
      "Iteration 216, loss = 0.02102845\n",
      "Iteration 217, loss = 0.02067546\n",
      "Iteration 218, loss = 0.02032727\n",
      "Iteration 219, loss = 0.01998886\n",
      "Iteration 220, loss = 0.01966048\n",
      "Iteration 221, loss = 0.01934117\n",
      "Iteration 222, loss = 0.01902692\n",
      "Iteration 223, loss = 0.01871736\n",
      "Iteration 224, loss = 0.01841789\n",
      "Iteration 225, loss = 0.01813222\n",
      "Iteration 226, loss = 0.01784403\n",
      "Iteration 227, loss = 0.01756403\n",
      "Iteration 228, loss = 0.01728982\n",
      "Iteration 229, loss = 0.01702321\n",
      "Iteration 230, loss = 0.01675870\n",
      "Iteration 231, loss = 0.01650119\n",
      "Iteration 232, loss = 0.01624657\n",
      "Iteration 233, loss = 0.01600128\n",
      "Iteration 234, loss = 0.01576529\n",
      "Iteration 235, loss = 0.01553044\n",
      "Iteration 236, loss = 0.01530096\n",
      "Iteration 237, loss = 0.01507817\n",
      "Iteration 238, loss = 0.01485958\n",
      "Iteration 239, loss = 0.01464313\n",
      "Iteration 240, loss = 0.01443468\n",
      "Iteration 241, loss = 0.01422735\n",
      "Iteration 242, loss = 0.01402676\n",
      "Iteration 243, loss = 0.01383188\n",
      "Iteration 244, loss = 0.01363889\n",
      "Iteration 245, loss = 0.01345053\n",
      "Iteration 246, loss = 0.01326372\n",
      "Iteration 247, loss = 0.01308151\n",
      "Iteration 248, loss = 0.01290368\n",
      "Iteration 249, loss = 0.01273135\n",
      "Iteration 250, loss = 0.01256178\n",
      "Iteration 251, loss = 0.01239677\n",
      "Iteration 252, loss = 0.01223270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 253, loss = 0.01207217\n",
      "Iteration 254, loss = 0.01191296\n",
      "Iteration 255, loss = 0.01176126\n",
      "Iteration 256, loss = 0.01161080\n",
      "Iteration 257, loss = 0.01146126\n",
      "Iteration 258, loss = 0.01131965\n",
      "Iteration 259, loss = 0.01117783\n",
      "Iteration 260, loss = 0.01104010\n",
      "Iteration 261, loss = 0.01090118\n",
      "Iteration 262, loss = 0.01076367\n",
      "Iteration 263, loss = 0.01062852\n",
      "Iteration 264, loss = 0.01049819\n",
      "Iteration 265, loss = 0.01036909\n",
      "Iteration 266, loss = 0.01024137\n",
      "Iteration 267, loss = 0.01011591\n",
      "Iteration 268, loss = 0.00999348\n",
      "Iteration 269, loss = 0.00987172\n",
      "Iteration 270, loss = 0.00975391\n",
      "Iteration 271, loss = 0.00963773\n",
      "Iteration 272, loss = 0.00952264\n",
      "Iteration 273, loss = 0.00940965\n",
      "Iteration 274, loss = 0.00929853\n",
      "Iteration 275, loss = 0.00919026\n",
      "Iteration 276, loss = 0.00908314\n",
      "Iteration 277, loss = 0.00897754\n",
      "Iteration 278, loss = 0.00887408\n",
      "Iteration 279, loss = 0.00877361\n",
      "Iteration 280, loss = 0.00867407\n",
      "Iteration 281, loss = 0.00857568\n",
      "Iteration 282, loss = 0.00847858\n",
      "Iteration 283, loss = 0.00838246\n",
      "Iteration 284, loss = 0.00828990\n",
      "Iteration 285, loss = 0.00819810\n",
      "Iteration 286, loss = 0.00810760\n",
      "Iteration 287, loss = 0.00801805\n",
      "Iteration 288, loss = 0.00793114\n",
      "Iteration 289, loss = 0.00784545\n",
      "Iteration 290, loss = 0.00776022\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(64, 32, 16), max_iter=1000, verbose=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp2.fit(x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac78541",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d1a51b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
